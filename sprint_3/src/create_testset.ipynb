{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# load .env file\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# get azure credentials from .env file\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "temperature = os.getenv(\"TEMPERATURE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Repos\\data2day\\data2day-rag-workshop\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai.chat_models import AzureChatOpenAI\n",
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "from ragas import evaluate\n",
    "\n",
    "azure_model = AzureChatOpenAI(\n",
    "    openai_api_version=api_version,\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=deployment_name,\n",
    "    api_key=api_key,\n",
    "    temperature=0,\n",
    "    validate_base_url=False,\n",
    ")\n",
    "\n",
    "# init the embeddings for answer_relevancy, answer_correctness and answer_similarity\n",
    "azure_embeddings = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=embedding_deployment_name,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error loading file ..\\data\\AI_auf_den_Markt_bringen_–_Fallstricke_&_Pflichten_.md: 'utf-8' codec can't decode byte 0xe2 in position 46: invalid continuation byte\n",
      "Error loading file ..\\data\\Data_Contracts_–_Der_Treiber_für_Automatisierung.md: 'utf-8' codec can't decode byte 0xe2 in position 36: invalid continuation byte\n",
      "Error loading file ..\\data\\Bring_your_own_Data_Hands-on_Enterprise_AI_Assistants_in_der_Cloud.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Datenanalysen_im_Fußball_–_ein_Überblick.md' is \"cannot open `..\\\\data\\\\datenanalysen_im_fu\\\\303\\\\237ball_\\\\342\\\\200\\\\223_ein_\\\\303\\\\234berblick.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Chatte_mit_deinen_Daten_Aufbau_eines_Retrieval-Augmented-Generation-(RAG)-Systems.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Datenarchitekturen_in_der_Realität_–_Was_passt_bei_uns.md' is \"cannot open `..\\\\data\\\\datenarchitekturen_in_der_realit\\\\303\\\\244t_\\\\342\\\\200\\\\223_was_passt_bei_uns.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Data_Mesh_Ein_Reality-Check.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Datenanalyse_mit_Machine_Learning.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Datenarchitekturen_in_der_Realität_–_Was_passt_bei_uns.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Datenanalysen_im_Fußball_–_ein_Überblick.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Datenkultur_bei_dm.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Der_Mensch_im_Fokus_nutzerzentriert,_vertrauenswürdige_KI-Technologie_entwickeln.md' is \"cannot open `..\\\\data\\\\der_mensch_im_fokus_nutzerzentriert,_vertrauensw\\\\303\\\\274rdige_ki-technologie_entwickeln.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\data\\\\Eine_Einführung_in_Large_Language_Models.md' is \"cannot open `..\\\\data\\\\eine_einf\\\\303\\\\274hrung_in_large_language_models.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Die_Engineering-Perspektive_auf_den_EU_AI_Act.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Der_Mensch_im_Fokus_nutzerzentriert,_vertrauenswürdige_KI-Technologie_entwickeln.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Eine_Einführung_in_Large_Language_Models.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Eine_globale_Daten_&_AI_Platform_in_der_Praxis.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Elasticsearch_Watcher_Hands-on_Ein_Use_Case_der_anderen_Art.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\How_to_build_reliable_data_pipelines_re-using_the_software_development’s_best_practices..md' is \"cannot open `..\\\\data\\\\how_to_build_reliable_data_pipelines_re-using_the_software_development\\\\342\\\\200\\\\231s_best_practices..md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Erfahrungen_mit_einer_End-to-End_MLOps-Pipeline_Praktische_Einblicke.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\How_to_build_reliable_data_pipelines_re-using_the_software_development’s_best_practices..md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Integrating_Local_LLMs_and_Knowledge_Graphs_for_Trustworthy_Legal_Chatbots.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\KI_Power-Play_Realistisches_Erwartungsmanagement_für_maximalen_Business_Impact.md' is \"cannot open `..\\\\data\\\\ki_power-play_realistisches_erwartungsmanagement_f\\\\303\\\\274r_maximalen_business_impact.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\data\\\\KI_zwischen_fantastischen_Möglichkeiten,_Rechtsunsicherheit_und_Disruption.md' is \"cannot open `..\\\\data\\\\ki_zwischen_fantastischen_m\\\\303\\\\266glichkeiten,_rechtsunsicherheit_und_disruption.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Keynote_Navigating_the_Future_Regulation_and_Its_Impact_on_AI_Practices.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\KI_Power-Play_Realistisches_Erwartungsmanagement_für_maximalen_Business_Impact.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\KI_zwischen_fantastischen_Möglichkeiten,_Rechtsunsicherheit_und_Disruption.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\LLM-MVC_Ein_Entwurfsmuster_zur_Strukturierung_von_LLM-Standardaufgaben_mit_generischen_Prompts.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Mit_unFIX_zur_Data_Company.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Praxisnahe_Erfahrungen_aus_dem_Data-Quality-Dschungel.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Road_to_Production_mit_Generative_AI_und_LLMs,_was_hat_sich_geändert.md' is \"cannot open `..\\\\data\\\\road_to_production_mit_generative_ai_und_llms,_was_hat_sich_ge\\\\303\\\\244ndert.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Python-Datenanalyse_Marimo_statt_Jupyter,_Polars_statt_Pandas,_Altair_statt_Matplotlib.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Road_to_Production_mit_Generative_AI_und_LLMs,_was_hat_sich_geändert.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Schluss_mit_Prototyp_GPTx_und_RAG_in_der_Praxis.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Semantische_Suche_und_ML_Das_passende_Dokument_finden.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\SiloX-GPT_Datensilos_verbinden_mittels_Multi-Agenten-System.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Size_recommender_a_data-driven_approach_to_fashion_sizing.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Strategic_Data_Methods_A_Safari_Tour_for_Data_People.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\The_Power_of_Knowledge_Graphs_and_AI_A_New_Era_for_Developers_and_Data_Scientists.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Transparente_Zeitreihenprognosen_Explainable_AI_(XAI)_für_Business_User_bei_der_BASF.md' is \"cannot open `..\\\\data\\\\transparente_zeitreihenprognosen_explainable_ai_(xai)_f\\\\303\\\\274r_business_user_bei_der_basf.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\data\\\\Von_Insellösungen_zu_einer_Datenplattform_–_Data_Mesh_pragmatisch_umsetzen_.md' is \"cannot open `..\\\\data\\\\von_insell\\\\303\\\\266sungen_zu_einer_datenplattform_\\\\342\\\\200\\\\223_data_mesh_pragmatisch_umsetzen_.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "The MIME type of '..\\\\data\\\\Urbane_Datenplattformen_für_Smarte_Städte.md' is \"cannot open `..\\\\data\\\\urbane_datenplattformen_f\\\\303\\\\274r_smarte_st\\\\303\\\\244dte.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\The_Rise_of_Modern_Data_Management.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Transparente_Zeitreihenprognosen_Explainable_AI_(XAI)_für_Business_User_bei_der_BASF.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Von_Sprachphilosophie_bis_zur_Spieltheorie_Was_wir_schon_immer_über_LLMs_wussten_.md' is \"cannot open `..\\\\data\\\\von_sprachphilosophie_bis_zur_spieltheorie_was_wir_schon_immer_\\\\303\\\\274ber_llms_wussten_.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Von_Insellösungen_zu_einer_Datenplattform_–_Data_Mesh_pragmatisch_umsetzen_.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "The MIME type of '..\\\\data\\\\Wieviel_KI_müssen_wir_unseren_Kunden_erklären.md' is \"cannot open `..\\\\data\\\\wieviel_ki_m\\\\303\\\\274ssen_wir_unseren_kunden_erkl\\\\303\\\\244ren.md' (no such file or directory)\". This file type is not currently supported in unstructured.\n",
      "Error loading file ..\\data\\Von_Sprachphilosophie_bis_zur_Spieltheorie_Was_wir_schon_immer_über_LLMs_wussten_.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Wieviel_KI_müssen_wir_unseren_Kunden_erklären.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n",
      "Error loading file ..\\data\\Urbane_Datenplattformen_für_Smarte_Städte.md: partition_md() is not available because one or more dependencies are not installed. Use: pip install \"unstructured[md]\" (including quotes) to install the required dependencies\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    \"../data\", use_multithreading=True, silent_errors=True)\n",
    "documents = loader.load()\n",
    "\n",
    "for document in documents:\n",
    "    document.metadata[\"filename\"] = document.metadata[\"source\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/Road_to_Production_mit_Generative_AI_und_LLMs,_was_hat_sich_geändert_.md', 'filename': '../data/Road_to_Production_mit_Generative_AI_und_LLMs,_was_hat_sich_geändert_.md'}, page_content='Road to Production mit Generative AI und LLMs, was hat sich geändert?\\n\\nEs hat sich in den letzten zwei Jahren viel getan im Bereich AI und maschinelles Lernen. Large Language Models liefern ganz neue Herangehensweisen, um intelligente Systeme zu bauen. Begannen früher Data- Science-Projekte mit der Datenakquise, so scheint es jetzt zu reichen, den richtigen Prompt zu schreiben.\\n\\nAber ist das wirklich wahr? Und wie sieht es aus mit der Komplexität, so ein Modell in Produktion zu bringen, wenn man nicht auf existierende SaaS-Angebote zurückgreifen will?\\n\\nIn diesem Vortrag vergleiche ich \"altes ML\" mit der neuen Welt im Hinblick auf den Weg in den Live-Betrieb und schaue mir an, was sich geändert hat und was geblieben ist – und ich gebe einen Überblick über das Angebot an Lösungen und Architekturen, um AI-Modelle live zu bringen.\\n\\nLernziele\\n\\nVerstehe die grundsätzlichen Unterschiede zwischen klassichen ML-Methoden und LLMs im Bezug auf den Produktiveinsatz.\\n\\nErhalte eine Übersicht über verschiedene Deploymentstrategien.\\n\\nVergleich von SaaS und Self-Hosted Deployment von LLMs.\\n\\nSpeaker\\n\\nMikio Braun ist Senior Principal Applied Scientist bei Zalando und arbeitet im Bereich von Fulfillment und Logistik. Neben seiner Tätigkeit als Data Scientist bei Zalando und GetYourGuide hat er in seiner Vergangenheit auch als Forscher im Bereich maschinelles Lernen gearbeitet und als Freiberufler Firmen und Teams geholfen, ML-basierte Lösungen zu bauen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/AI_auf_den_Markt_bringen_–_Fallstricke_&_Pflichten_.md', 'filename': '../data/AI_auf_den_Markt_bringen_–_Fallstricke_&_Pflichten_.md'}, page_content='AI auf den Markt bringen – Fallstricke & Pflichten\\n\\nDas Modell ist fertig, die GPUs laufen sich warm, die Kunden können kommen. Was ist noch zu beachten, damit das AI-basierte Produkt auch ein unternehmerischer Erfolg wird?\\n\\nDieser Vortrag beleuchtet drei Schwerpunktthemen, die man beachten muss, um einen Misserfolg zu verhindern.\\n\\nRegulierung: Welche Vorarbeit muss man leisten, um nicht gegen AI Act und andere Pflichten zu verstoßen?\\n\\nSicherheit & Qualität: Wie stellt man sicher, dass Modelle nicht angegriffen und missbraucht werden können?\\n\\nWirtschaftlichkeit: AI ist teuer. Nicht jede Form von Machine Learning rechnet sich notwendigerweise auch vor Kunde.\\n\\nDer Vortrag hilft dabei, einen Rundumblick für sein AI-Produkt zu erhalten. Insbesondere werden dabei Aspekte berücksichtigt, die in Zeiten von Machine Learning und AI neu sind.\\n\\nSpeaker\\n\\nBernd Fondermann hilft bei der Umsetzung von Datenstrategien und dem Aufbau von Datenplattformen. Er unterstützt seine Kunden dabei, die Bereiche Produktmanagement, datengetriebene Ansätze und Software Engineering erfolgreich zusammenzuführen. Seit Kurzem ist er auch als Berater im Bereich der KI-Regulierung tätig. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Bring_your_own_Data__Hands-on_Enterprise_AI_Assistants_in_der_Cloud.md', 'filename': '../data/Bring_your_own_Data__Hands-on_Enterprise_AI_Assistants_in_der_Cloud.md'}, page_content='Bring your own Data: Hands-on Enterprise AI Assistants in der Cloud\\n\\nIn diesem Workshop nehmen wir eine hands-on Perspektive zur Bewertung von verschiedenen LLM-Varianten (GPT3.5 & 4, Aleph-Alpha, Llama, Mixtral, …), Methoden (RAG, …) und Plattformen ein.\\n\\nBeginnend mit einer Einführung in die notwendigen technologischen Komponenten vergleichen wir die verfügbaren Optionen, basierend auf Azure, AWS, Google Cloud, Snowflake sowie Databricks.\\n\\nAnschließend wählen wir einen geeigneten Stack aus und implementieren Schritt für Schritt die Module eines KI-Assistenten (Ansprechen der APIs, Integration in Applikationen, Datenanbindung, …). Dabei diskutieren wir den Umgang mit zentralen Aspekten wie Prompting, Metriken zur Evaluierung und Umgang mit Halluzinationen.\\n\\nVorkenntnisse\\n\\ngrundlegende Programmierkenntnisse (bspw. in Python)\\n\\nerste Erfahrungen im Umgang KI-Assistenten\\n\\nUmgang mit einer Entwicklungsumgebung / IDE\\n\\nAffinität im Umgang mit Daten\\n\\nerste Erfahrungen im Umgang mit Cloud-Ressourcen\\n\\nLernziele\\n\\nVerständnis der beteiligten logischen und technischen Komponenten eines AI Assistants\\n\\nlauffähige Implementierung eines angepassten AI Assistenten\\n\\nBest Practices zum Umgang mit zentralen Aspekten wie Prompt Engineering, Evaluierung\\n\\nVerständnis der noch notwendigen Schritte zur Produktivierung\\n\\nAgenda\\n\\nPausenzeiten * ab 09:00 Uhr: Registrierung und Begrüßungskaffee * 10:00 Uhr: Beginn * 12:30 - 13:30 Uhr: Mittagspause * 15:00 - 15:15 Uhr: Kaffeepause * 16:15 - 16:30 Uhr: Kaffeepause * ca. 17:00 Uhr: Ende\\n\\nSpeaker\\n\\nSebastian Blank beschäftigt sich als Head of Natural Language Processing bei inovex mit der Identifizierung und Entwicklung von Datenprodukten für natürliche Sprache. Er interessiert sich besonders für den Einsatz großer Sprachmodelle, ihren Mehrwert auf realen Anwendungsfällen mit wenig gelabelten Daten und die Herausforderungen, solche Modelle in Produktion zu bringen. __@databenz Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/KI_Power-Play__Realistisches_Erwartungsmanagement_für_maximalen_Business_Impact.md', 'filename': '../data/KI_Power-Play__Realistisches_Erwartungsmanagement_für_maximalen_Business_Impact.md'}, page_content='KI Power-Play: Realistisches Erwartungsmanagement für maximalen Business\\n\\nImpact Mit dem Hype um GenAI stehen wir als Data Scientists vor der Herausforderung, die hohen (und manchmal unrealistischen) Erwartungen an KI zu managen.\\n\\nEs braucht die richtigen Herangehensweisen, um mit Daten und KI echten Business Impact zu schaffen, und um somit Enttäuschungen zu vermeiden. Dabei ist es wichtig effiziente Kollaboration zwischen Data Scientists und Business Experten zu ermöglichen. Letztendlich können wir dann sogar GenAI nutzen, um uns bei dieser Herausforderung zu unterstützen.\\n\\nSpeaker\\n\\nKira Engelhardt studierte Data Science an der LMU in München und verfügt über mehr als ein Jahrzehnt relevante Erfahrung in der Transformation wichtiger Geschäftsprozesse durch Data Science, Daten und KI. Sie begann ihre Karriere in der Finanzbranche und arbeitete als Data Scientist bei HUK Coburg, KPMG und der Allianz. Nach mehreren Jahren Erfahrung als KI-Projektleiterin in Deutschland und Schweden bei E.ON leitet sie nun das globale Data- und KI- Inhouse-Consulting-Team für Energy Networks bei E.ON. Ihr Ziel ist es, die Digitalisierung unserer Strom- und Gasnetze voranzutreiben und so die Energiewende zu ermöglichen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Data_Contracts_–_Der_Treiber_für_Automatisierung.md', 'filename': '../data/Data_Contracts_–_Der_Treiber_für_Automatisierung.md'}, page_content='Data Contracts – Der Treiber für Automatisierung\\n\\nIn modernen verteilten Datenarchitekturen, wie z. B. Data Mesh, werden Daten zunehmend zwischen verschiedenen Teams ausgetauscht. Wir brauchen eine Möglichkeit, uns auf die Qualität und Stabilität der von uns verwendeten Daten zu verlassen.\\n\\nData Contracts sind so ähnlich wie OpenAPI-Spezifikationen, aber die Datenwelt funktioniert etwas anders. Sie definieren in einem YAML-Format das logische Schema der bereitgestellten Daten, aber auch Semantik, Qualitätsattribute, SLAs, und vor allem Ownership.\\n\\nIn diesem Talk zeigt Jochen, wie man mit Data Contracts anforderungsgetrieben arbeiten kann, aber auch, wie man Data Contracts nutzen kann, um Tests und Governance zu automatisieren.\\n\\nVorkenntnisse\\n\\nGrundlagen von Data Mesh sind hilfreich, aber nicht notwendig.\\n\\nLernziele\\n\\nData Contracts für die anforderungsgetriebene Entwicklung\\n\\nData Contracts zum Testen verwenden\\n\\nDas Tool Data Contract CLI kennenlernen\\n\\nSpeaker\\n\\nJochen Christ ist Tech Lead bei INNOQ und ist Spezialist für Data Mesh. Jochen arbeitet momentan hauptsächlich am Data Mesh Manager (datamesh- manager.com). Er ist (Co-)Autor von datamesh-architecture.com und datamesh- governance.com, sowie Übersetzer von Zhamak Dehghanis Data-Mesh-Buch. __@jochen_christ Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Erfahrungen_mit_einer_End-to-End_MLOps-Pipeline__Praktische_Einblicke.md', 'filename': '../data/Erfahrungen_mit_einer_End-to-End_MLOps-Pipeline__Praktische_Einblicke.md'}, page_content='Erfahrungen mit einer End-to-End MLOps-Pipeline: Praktische Einblicke\\n\\nDie Implementierung einer End-to-End MLOps-Pipeline ist ein komplexer Prozess, der viele Herausforderungen und Lernmöglichkeiten bietet.\\n\\nIn diesem Vortrag werden konkrete Erfahrungen bei der Entwicklung und Umsetzung einer End-to-End MLOps-Pipeline geteilt.\\n\\nWir beleuchten die praktischen Aspekte von der Datenvorbereitung über das Modelltraining bis hin zur Bereitstellung und Überwachung in produktiven Umgebungen. Dabei werden wichtige Entscheidungen, Herausforderungen und Erfolgsfaktoren, die bei der Implementierung einer solchen Pipeline auftreten können, diskutiert. Darüber hinaus werden bewährte Praktiken und Empfehlungen für einen reibungslosen Betrieb vorgestellt.\\n\\nVorkenntnisse\\n\\nBesucher sollten grundlegende Kenntnisse im Bereich maschinelles Lernen und Datenanalyse mitbringen sowie ein Verständnis für MLOps-Konzepte und Erfahrung mit Datenpipelines haben. Ein Interesse an praktischen Erfahrungen und Fallstudien zur Umsetzung von MLOps-Pipelines ist von Vorteil.\\n\\nLernziele\\n\\nDie Teilnehmenden sollen verstehen, * wie eine End-to-End MLOps-Pipeline entwickelt und implementiert wird. * Sie sollen praktische Einblicke in die Datenvorbereitung, Modelltraining, Bereitstellung und Überwachung erhalten sowie * bewährte Praktiken zur Optimierung des MLOps-Prozesses kennenlernen.\\n\\nSpeaker\\n\\nRené Brunner ist ein erfahrener Data Scientist und Ingenieur mit Leidenschaft für die Entwicklung von innovativen Lösungen im Bereich des maschinellen Lernens und der künstlichen Intelligenz. Mit einem starken Hintergrund in Informatik und Datenanalyse verfügt Prof. Dr. René Brunner über umfassende Kenntnisse in der Entwicklung und Implementierung von MLOps- Pipelines für Unternehmen verschiedener Branchen. Eric Joachim Liese arbeitete nach seinem Abschluss in Mathematik und Informatik mit Schwerpunkt auf maschinellem Lernen und KI mehrere Jahre lang als Senior Data Scientist, ML Engineer und Berater. Er entwarf Konzepte zur Automatisierung der Entwicklung von KI-Produkten durch die Erstellung von End- to-End MLOps Pipelines sowie Strategien, die Unternehmen dabei helfen, daten- und KI-gesteuert zu werden. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/The_Rise_of_Modern_Data_Management.md', 'filename': '../data/The_Rise_of_Modern_Data_Management.md'}, page_content='The Rise of Modern Data Management\\n\\nIn this session, Chad Sanderson, CEO of Gable.ai and author of the upcoming O\\'Reilly book \"Data Contracts,\" tackles the necessity of modern data management in an age of hyper iteration, experimentation, and AI. He will explore why traditional data management practices fail and how the cloud has fundamentally changed data development.\\n\\nThe talk will cover a modern application of data management best practices, including data change detection, data contracts, observability, and CI/CD tests, and outline the roles of data producers and consumers.\\n\\nAttendees will leave with a clear understanding of modern data management\\'s components and how to leverage them for better data handling and decision- making.\\n\\nLernziele\\n\\nImplement Data Contracts: Establish and enforce data contracts between data producers and consumers to ensure data quality and compatibility, facilitating smoother data integration and use across different systems and teams.\\n\\nAdopt Observability in Data Systems: Integrate observability tools and practices into your data management strategy to gain real-time insights into your data\\'s health, usage, and performance, enabling proactive issue resolution and optimization.\\n\\nApply CI/CD Practices to Data Development: Utilize Continuous Integration and Continuous Deployment (CI/CD) methodologies for data pipelines and datasets to accelerate updates, improve data accuracy, and enhance collaboration between data teams.\\n\\nSpeaker\\n\\nChad Sanderson is the CEO & Co-Founder of Gable, a collaboration, communication, and change management platform for data teams operating at scale. He is a prominent figure in the data tech industry, having held key data positions at leading companies such as Convoy, Microsoft, Sephora, Subway, and Oracle. Chad is also the author of the upcoming O\\'Reilly book, \"Data Contracts\" and writes about the future of data infrastructure, modeling, and contracts in his newsletter \"Data Products\". Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Von_Insellösungen_zu_einer_Datenplattform_–_Data_Mesh_pragmatisch_umsetzen_.md', 'filename': '../data/Von_Insellösungen_zu_einer_Datenplattform_–_Data_Mesh_pragmatisch_umsetzen_.md'}, page_content='Von Insellösungen zu einer Datenplattform – Data Mesh pragmatisch umsetzen\\n\\nDer Aufbau einer unternehmensweiten Datenplattform, die sowohl Agilität für Fachbereiche als auch transparente Governance bietet, stellt viele Unternehmen vor Herausforderungen. Mit Data Mesh kam vor einigen Jahren ein Ansatz auf, der auf verständlichen Prinzipien aufbaut, jedoch schwer in der Realität umzusetzen ist.\\n\\nIn dem Vortrag zeigen wir unseren Weg von Insellösungen hin zu einer bereichsübergreifenden Datenarchitektur und stellen dabei die Perspektiven der Fachbereiche und einer zentralen IT gegenüber. Wir gehen auf die Aufteilung der Ende-zu-Ende Verantwortung zwischen Teams ein und leiten daraus unsere prozessualen und technologischen Entscheidungen ab.\\n\\nVorkenntnisse\\n\\nInteresse an Datenarchitektur und Datenmanagement\\n\\nIdealerweise Erfahrung mit Implementierung und Betreiben von datengestützten Kundenlösungen\\n\\nLernziele\\n\\nNeue Ansätze zum Vorgehen in eigenen Projekten im Bereich der Datenarchitektur kennenlernen\\n\\nWechselwirkung zwischen Organisation und Datenarchitektur nachvollziehen\\n\\nSpeaker\\n\\nSven Walleser startete 2014 als Entwicklungsingenieur in der Produktentwicklung der RATIONAL AG und beschäftigt sich seit 2018 mit der Vernetzung der Kundengeräte und der datenbasierten Produktentwicklung im internationalen Kundendienst. Mittlerweile ist er als Director Data Solutions verantwortlich für die Entwicklung und Implementierung der digitalen Service Produkte. Rostislaw Krassow verantwortet als Data Architect bei der RATIONAL AG die unternehmensweite Data-&-Analytics-Plattform. Abwägung unterschiedlicher Interessen und Vermittlung hin zu einer gemeinsamen Lösung stehen für ihn dabei im Mittelpunkt. Auf der Technologieseite liegt sein Fokus auf verteilten Datenbanken, Apache-Hadoop-Ökosystem und verwandten Cloud-Diensten. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/How_to_build_reliable_data_pipelines_re-using_the_software_development’s_best_practices..md', 'filename': '../data/How_to_build_reliable_data_pipelines_re-using_the_software_development’s_best_practices..md'}, page_content='How to build reliable data pipelines re-using the software development’s\\n\\nbest practices. You have been solicited to implement a new data pipeline architecture. The aim: turn wealth of raw data into friendly off-the-shelf data inputs for the data science, analytics and reporting teams.\\n\\nFollowing the software development’s best practices, you will learn how to enforce a successful workflow, distinguish between bad and good software implementations, escape pitfalls and enable data to be served across the company.\\n\\nOverview on the Software Development Best Practices (Don’t Repeat Yourself, Decouple, Design by Contract, Crash Early)\\n\\nHands-on Example: A Python Data Pipeline (Standardisation: black, mypy and pylint formatting; Testing & Documentation: Unittests; Automation: CI/CD Integration)\\n\\nVorkenntnisse\\n\\nThe target group are entry-level data engineers/scientists/analysts looking for conceptual ideas and best practices on how to implement data pipelines, re-using the best practices of software development.\\n\\nLernziele\\n\\nUnderstand the benefits of adopting the best principles of software development in your systems;\\n\\nLearn to visualize, through a concrete example, what these best principles could be, and the various trade-offs to be taken into account;\\n\\nLearn how to implement these principles in your production environment.\\n\\nSpeaker\\n\\nOlivier Bénard is a Data Engineer in the retail industry. He takes care of Google Cloud data platform, builds ETL/ELT data pipelines and the cloud infrastructure for them to shape the scalable future of data-driven work in this industry. His background lies in Software Engineering. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Praxisnahe_Erfahrungen_aus_dem_Data-Quality-Dschungel.md', 'filename': '../data/Praxisnahe_Erfahrungen_aus_dem_Data-Quality-Dschungel.md'}, page_content='Praxisnahe Erfahrungen aus dem Data-Quality-Dschungel\\n\\nUnter Datenqualität versteht man den Grad der Übereinstimmung von Daten mit den durch sie repräsentierten realen Dingen oder Sachverhalten.\\n\\nDa es in der Praxis oft schwierig ist, die Qualität von Daten anhand dieser Definition zu beurteilen, wird versucht, die Qualität von Daten über die Abweichung von zuvor definierten Annahmen zu beurteilen, z.B. ob der Temperaturwert eines Sensors in einem erwarteten Wertebereich liegt und dessen Einheiten nur aus einem vordefinierten Set [\"Celsius\", \"Fahrenheit\"] stammen. Qualitative Daten sind für Datenprodukte von immenser Bedeutung, da sie z.B. über Schnittstellen bereitgestellt werden, oder die Prognosequalität von ML- Modellen beeinflussen.\\n\\nIn unserem Vortrag wollen wir neben Data-Quality-Grundlagen von unseren Praxis-Erfahrungen bei der Verwendung der Data Quality Frameworks Soda und Great Expectations berichten.\\n\\nVorkenntnisse\\n\\nGrundlegendes Verständnis von Data Engineering\\n\\nGrundlegendes Verständnis von Datenverarbeitung mit Python/PySpark\\n\\nLernziele\\n\\nWichtigkeit von guter Datenqualität verstehen\\n\\nHerausforderungen und Lösungsansätze bei der Sicherstellung von Data Quality verstehen\\n\\nGrundlegendes Verständnis von Great Expectations und Soda und deren Vor- und Nachteile\\n\\nSpeaker\\n\\nFlorian Gräbe hat am KIT Wirtschaftinsgenieurwesen studiert. Aktuell arbeitet er bei inovex als Data- und ML Engineer und setzt dort Datenprojekte in der Cloud für Kunden aus verschiedensten Branchen um. Marcel Spitzer ist Data Engineer bei inovex. Er beschäftigt sich mit der Entwicklung von Streaming- und Batch-Pipelines zur Datenverarbeitung in verteilten Systemen und nutzt Machine Learning um Datenprodukte smart zu machen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Von_Sprachphilosophie_bis_zur_Spieltheorie__Was_wir_schon_immer_über_LLMs_wussten!_.md', 'filename': '../data/Von_Sprachphilosophie_bis_zur_Spieltheorie__Was_wir_schon_immer_über_LLMs_wussten!_.md'}, page_content='Von Sprachphilosophie bis zur Spieltheorie: Was wir schon immer über LLMs\\n\\nwussten! \"Die Grenzen meiner Sprache bedeuten die Grenzen meiner Welt.\" – Ein berühmtes Zitat von Ludwig von Wittgenstein, einem Philosophen, der versuchte, die Beziehung zwischen Sprache, Gedanken und Realität zu bestimmen. Er beendete sein Lebenswerk mit der Beobachtung: \"Die Bedeutung eines Wortes ist sein Gebrauch in der Sprache.\" Das klingt doch verdächtig nach Large Language Modellen, oder?\\n\\nWarum begeistern uns diese Modelle eigentlich so? Was unterscheidet unsere Agenten-Systeme eigentlich von Executive- und Task-Networks im Gehirn? Wie kann uns Sprache helfen, chaotische Systeme zu bezwingen und das Wetter vorherzusagen? Was hat die Auswahl von Datensätzen mit Oligopolmärkten zu tun?\\n\\nDieser Vortrag bietet einen Einblick in Erkenntnisse, die andere Disziplinen schon immer kannten, und offenbart, was wir in der künstlichen Intelligenz noch lernen müssen!\\n\\nLernziele\\n\\nEin breiteres Verständnis über die Theorie von LLMs.\\n\\nSpeaker\\n\\nJulian Kurz ist Lead Expert Generative AI bei der HMS Analytical Software GmbH. Er hat seinen Master in Künstlicher Intelligenz mit dem Fokus auf Natural Language Processing (NLP) in Edinburgh gemacht. Seitdem hat er viele Projekte in den Bereichen Machine Learning, Deep Learning, IoT und Cloud Computing umgesetzt. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Keynote__Navigating_the_Future__Regulation_and_Its_Impact_on_AI_Practices.md', 'filename': '../data/Keynote__Navigating_the_Future__Regulation_and_Its_Impact_on_AI_Practices.md'}, page_content=\"Keynote: Navigating the Future: Regulation and Its Impact on AI Practices\\n\\nThe European Union's AI Act is set to revolutionize the landscape of artificial intelligence development and deployment. This talk explores the implications of this groundbreaking legislation for data scientists and AI executives, focusing on how it will reshape practices and promote responsible AI.\\n\\nWe'll delve into key aspects of the EU AI Act, including risk categorization, transparency requirements, and accountability measures. By attending this talk, data scientists will learn how to future-proof their skills and methodologies, while AI executives will gain valuable knowledge to guide their organizations through this regulatory shift. Don't miss this opportunity to stay ahead of the curve and shape the future of responsible AI in your organization.\\n\\nSpeaker\\n\\nLexy Kassan is a Lead Data and AI Strategist at Databricks, overseeing and guiding customer organizations through holistic transformation programmes built on industry trends and best practices. As an Ethicist, Lexy coaches companies on implementing responsible AI and works with public affairs to shape Databricks’ messaging to regulators and lobbyists. Jetzt Tickets sichern\"),\n",
       " Document(metadata={'source': '../data/SiloX-GPT__Datensilos_verbinden_mittels_Multi-Agenten-System.md', 'filename': '../data/SiloX-GPT__Datensilos_verbinden_mittels_Multi-Agenten-System.md'}, page_content='SiloX-GPT: Datensilos verbinden mittels Multi-Agenten-System\\n\\nFirmen streben seit Langem danach, Datensilos zu durchbrechen, um einen allumfassenden Data Lake zu schaffen – bisher meist ohne anhaltenden Erfolg. Der Fortschritt großer Sprachmodelle ermöglicht nun, Daten verschiedenster Art und Struktur leicht abfragbar zu machen.\\n\\nBertelsmann hat zusammen mit scieneers daher einen Versuch unternommen, die Daten in ihren unterschiedlichen Quellsystemen (Graph-Datenbanken, Web- Services, Volltext …) zu belassen, aber mittels eines Multi-Agenten-Systems in LangGraph über eine einfache Chat-Oberfläche universell zugänglich zu machen.\\n\\nLernziele\\n\\nPotenziale und Probleme beim Einsatz großer Sprachmodelle für den Zugriff auf Daten unterschiedlichster Struktur verstehen.\\n\\nEinblick in die Erstellung von Sprachagenten mittels LangChain.\\n\\nVerständnis dafür erlangen, was ein Multi-Agenten-System ausmacht und wie es in LangGraph umgesetzt wird.\\n\\nWeitergabe von Lessons Learned.\\n\\nSpeaker\\n\\nNico Kreiling ist Data Scientist @ scieneers & Host des deutschen Entwickler-Podcasts Techtiefen. __@krlng Moritz Glauner ist Head of Data Science at Bertelsmann Data Services. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Data_Mesh__Ein_Reality-Check.md', 'filename': '../data/Data_Mesh__Ein_Reality-Check.md'}, page_content='Data Mesh: Ein Reality-Check\\n\\nIn diesem Vortrag berichten wir über die Einführung eines Data Mesh bei einem globalen Maschinenbau-Konzern.\\n\\nWir geben zunächst einen kurzen Einblick in den Tech-Stack, der aus einem Data Lakehouse in Azure mit Daten-Pipelines in PySpark besteht. Anschließend beleuchten wir die sozio-kulturellen Aspekte der Data-Mesh-Einführung. Dabei gehen wir auf Widerstände aus den Domänenteams ein, die u.a. durch fehlende Zeit und Data-Engineering-Skills bedingt sind.\\n\\nWir teilen, was gut und was weniger gut funktioniert hat und was wir dabei gelernt haben. Außerdem diskutieren wir mit dem Publikum Ansätze, um die Verständnislücke bezüglich des Mehrwerts des Data Mesh bei den Teams zu schließen und alle Stakeholder bestmöglich mitzunehmen.\\n\\nVorkenntnisse\\n\\nEin grundlegendes Verständnis über den Aufbau von Cloud-Datenplattformen. Erfahrungen mit Data Mesh sind hilfreich, aber nicht nötig.\\n\\nLernziele\\n\\nTeilnehmerinnen und Teilnehmer erfahren, welche Herausforderungen bei der Einführung eines Data Mesh in der Praxis auftauchen können und Ansätze, wie man sie lösen kann.\\n\\nSpeaker\\n\\nFilip Stepniak ist seit 6 Jahren bei esentri im Bereich Industrial Analytics & IoT. Er spricht die Sprache der Data Strategists und Data Engineers. Seit einem Jahr steht er vor der Herausforderung, das Data-Mesh- Konzept beim Kunden umzusetzen. Nicolas Haubner ist seit 4 Jahren bei esentri im Bereich Enterprise Analytics & Sustainability. Dr. Haubner hat Erfahrung aus diversen Kundenprojekten im Data Engineering, sowohl im Aufbau von zentralen Datenplattformen als auch in der Einführung des Data Mesh. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Elasticsearch_Watcher_Hands-on__Ein_Use_Case_der_anderen_Art.md', 'filename': '../data/Elasticsearch_Watcher_Hands-on__Ein_Use_Case_der_anderen_Art.md'}, page_content='Elasticsearch Watcher Hands-on: Ein Use Case der anderen Art\\n\\nIn der dynamischen Welt der Daten sind gute Werkzeuge zum Monitoring und Alerting unverzichtbar. Viele Tools bieten Möglichkeiten, Daten aus verschiedenen Systemen zu sammeln und zugänglich zu machen – so auch der Elastic Stack. Transformation und Aggregation kann dabei an verschiedenen Stellen passieren. Ein Werkzeug innerhalb des Elastic Stacks, das sicher nicht primär als Datentransformator bekannt ist, ist die Funktion \"Watcher\". Diese wird meist lediglich für Alerting eingesetzt.\\n\\nIn diesem Vortrag führe ich Sie in die Grundlagen von Watcher ein und präsentiere Ihnen praxisnah einen eher ungewöhnlichen Anwendungsfall für Watcher, den wir in einem unserer Projekte erfolgreich umsetzen konnten. Bei diesem werden Statuswerte verschiedener Systeme mittels Watcher nicht nur überwacht, sondern so zu neuen Datentöpfen zusammengeführt, dass wir den Gesamtstatus des Systems performant und mit wenig zeitlichen Versatz übersichtlich darstellen können. Dafür nutzen wir innerhalb von Watcher Suchlogik und Re-Indexierung.\\n\\nIch möchte durch unsere Erfahrungen mit Watcher (zukünftigen) Elasticsearch- Nutzern eine neue Perspektive auf das Feature geben und Sie ermutigen, Watcher auch für Ihre Probleme im Bereich Datenverarbeitung zu nutzen.\\n\\nVorkenntnisse\\n\\nGrundlegende Kenntnisse von Elasticsearch und Kibana sind von Vorteil.\\n\\nLernziele\\n\\nZiel ist es, den Zuhörern einen guten Eindruck zu vermitteln, was Elasticsearchs Feature \"Watcher\" leistet. Dabei möchte ich einerseits auf die klassischen Use Cases wie Monitoring und Alerting eingehen, aber auch vor allem die Flexibilität von Watcher und die daraus resultierenden breiten Einsatzmöglichkeiten vermitteln.\\n\\nSpeaker\\n\\nBianca Schlüter unterstützt als Senior Consultant Search & Analytics bei der SHI GmbH seit fast vier Jahren Unternehmen bei der Integration und Optimierung von Such- und Analytics-Lösungen auf Basis moderner Open-Source- Technologien. Zuvor absolvierte sie ein Studium der Mathematik mit den Schwerpunkten Statistik und Optimierung, in dessen Verlauf sie bereits ihrer Begeisterung für Daten nachgehen konnte. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Transparente_Zeitreihenprognosen__Explainable_AI_(XAI)_für_Business_User_bei_der_BASF.md', 'filename': '../data/Transparente_Zeitreihenprognosen__Explainable_AI_(XAI)_für_Business_User_bei_der_BASF.md'}, page_content='Transparente Zeitreihenprognosen: Explainable AI (XAI) für Business User bei\\n\\nder BASF Warum hat sich die Wettervorhersage für heute schon wieder geändert?\\n\\nDas fragen sich viele. Aber nicht nur Wettervorhersagen, auch Zeitreihenprognosen können sich im Laufe der Zeit ändern. Doch warum ist das so?\\n\\nDiese Frage unserer Business User der Forecasting-Plattform bei BASF war eine unserer Hauptmotivationen, uns mit dem Einsatz von Explainable AI (XAI) für Zeitreihenprognosen zu beschäftigen.\\n\\nSHAP-Werte, ein Ansatz aus der kooperativen Spieltheorie, ermöglichen es, individuelle Vorhersagen zu erklären. Sie quantifizieren die Größe und Richtung (positiv oder negativ) des Einflusses einer Funktion auf eine Vorhersage. Bei Zeitreihenprognosen helfen SHAP-Werte, die Unterschiede zwischen verschiedenen Modellen und Methoden zu verstehen und die treibenden Variablen hinter den unterschiedlichen Vorhersagen zu identifizieren.\\n\\nDa unsere Business User eine verständliche Erklärung benötigen, war unser nächster Schritt, die Informationen aus den SHAP-Werten auf ihre wesentlichen Punkte zu reduzieren. Gemeinsam mit den Anwenderinnen und Anwendern haben wir schließlich eine optimale Darstellung entwickelt. Das Ergebnis ist eine XAI- Erweiterung unserer Forecasting-Plattform, die die Vorhersagen für die Business User transparent und nachvollziehbar macht.\\n\\nVorkenntnisse\\n\\nGrundlegendes Verständnis des Vorgehens bei Machine Learing.\\n\\nLernziele\\n\\nLernziel ist ein Verständnis von Interpretation von Zeitreihenvorhersagen mittels SHAP-Werten zu erlangen, sowie deren Möglichkeiten und derzeitigen Beschränkungen zu verstehen.\\n\\nSpeaker\\n\\nMarkward Britsch ist Senior Data Scientist bei HMS Analytical Software in Heidelberg. Er promovierte in Astrophysik und setzte während seiner Post-Doc- Zeit in der experimentellen Teilchenphysik am CERN Machine Learning zur Analyse von LHC-Daten ein. In seiner derzeitigen Rolle konzentriert er sich auf die Herausforderungen analytischer Projekte, die tiefgehende Kenntnisse in Datenanalyse und Machine Learning erfordern. Stephan Sauer ist Head of Forecast Solutions bei der BASF Digital Solutions GmbH. Seine Verantwortung umfasst die strategische Entwicklung und den Betrieb der internen Forecasting-Plattform von BASF sowie deren praktische Anwendung und das Enablement innerhalb der BASF-Gruppe und -Funktionen wie Controlling, Finance, Sales & Marketing, Supply Chain und anderen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Datenanalyse_mit_Machine_Learning.md', 'filename': '../data/Datenanalyse_mit_Machine_Learning.md'}, page_content='Datenanalyse mit Machine Learning\\n\\nIn diesem Workshop führen wir ein Analyseprojekt durch und nutzen dazu unterschiedliche Methoden.\\n\\nBasis bilden unstrukturierte Daten aus dem Technology-Subreddit, die wir zunächst statistisch analysieren. Anschließend werden wir die Datenmenge mittels Klassifikation einengen und versuchen, die Inhalte genauer zu verstehen. Dazu kommt Topic Modeling als unüberwachtes Machine Learning im Bereich NLP zum Einsatz.\\n\\nSchließlich werden wir die Sentiments mit LLMs bestimmen. Daraus ergeben sich Zeitserien, die wir analysieren die zukünftige Entwicklung mit unterschiedlichen Methoden vorhersagen.\\n\\nVorkenntnisse\\n\\nPython wäre hilfreich, muss aber nicht unbedingt sein Wer seinen eigenen Computer nutzen will, sollte Jupyter installieren, es geht aber auch mit Colab.\\n\\nLernziele\\n\\nKennenlernen unterschiedlicher Datenanalysemethoden\\n\\nÜberwachtes und unüberwachtes Machine Learning\\n\\nLLMs und Transformer\\n\\nZeitserien-Vorhersagen\\n\\nProjektablauf in komplexen Analyseprojekten kennenlernen\\n\\nAgenda\\n\\nPausenzeiten * ab 09:00 Uhr: Registrierung und Begrüßungskaffee * 10:00 Uhr: Beginn * 12:30 - 13:30 Uhr: Mittagspause * 15:00 - 15:15 Uhr: Kaffeepause * 16:15 - 16:30 Uhr: Kaffeepause * ca. 17:00 Uhr: Ende\\n\\nTechnische Anforderungen\\n\\nLaptop (mit Python/Jupyter oder alternativ Google Colab)\\n\\nSpeaker\\n\\nChristian Winkler beschäftigt sich seit vielen Jahre mit künstlicher Intelligenz, speziell in der automatisierten Analyse natürlichsprachiger Texte (NLP). Als Professor an der TH Nürnberg konzentriert sich seine Forschung auf die Optimierung von User Experience mithilfe moderner Verfahren. Er forscht und publiziert zu AI/NLP und ist regelmäßig Sprecher auf Konferenzen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Chatte_mit_deinen_Daten__Aufbau_eines_Retrieval-Augmented-Generation-(RAG)-Systems.md', 'filename': '../data/Chatte_mit_deinen_Daten__Aufbau_eines_Retrieval-Augmented-Generation-(RAG)-Systems.md'}, page_content='Chatte mit deinen Daten: Aufbau eines Retrieval-Augmented-\\n\\nGeneration-(RAG)-Systems In diesem Workshop werden die Teilnehmer dazu befähigt, ein Retrieval- Augmented-Generation-System (RAG) zu entwickeln, das Anfragen gegen eine umfangreiche Textdatenbank bearbeitet.\\n\\nDurch den Aufbau eines Embeddings-basierten Index und die kontextbasierte Beantwortung von Fragen anhand relevanter Dokumente lernen die Teilnehmer:innen, wie sie ihre Daten in eine interaktive Konversation verwandeln können.\\n\\nVorkenntnisse\\n\\nDie Zielgruppe für diesen Workshop sind Entwickler, die entweder noch keine oder nur geringe praktische Erfahrung mit Large Language Models (LLMs) haben und über grundlegende Python-Kenntnisse verfügen.\\n\\nLernziele\\n\\nDaten für und mit Large Language Models (LLMs) aufarbeiten und persistieren: Die Teilnehmer werden lernen, wie man Daten für die Verarbeitung mit LLMs vorbereitet und wie man sie effektiv persistiert, um den Trainingsprozess zu unterstützen.\\n\\nVerarbeitung von (Text)daten mit LLMs: Die Teilnehmer werden die Grundlagen der Verarbeitung von Textdaten mit LLMs kennenlernen. Dies umfasst das Einlesen, Vorbereiten und Anwenden von LLMs auf Textdaten.\\n\\nErfahrungen mit Large Language Models sammeln: Durch praktische Übungen und Beispiele werden die Teilnehmer Erfahrungen im Umgang mit LLMs sammeln, um ein besseres Verständnis für deren Funktionsweise und Anwendungsmöglichkeiten zu entwickeln.\\n\\nAgenda\\n\\nab 09:00 Uhr: Registrierung und Begrüßungskaffee\\n\\n10:00 Uhr: Beginn\\n\\nVorstellung RAG System\\n\\nPräsentation RAG Komponenten\\n\\nStreamlit UI\\n\\nLangchain Backend\\n\\nAzure OpenAI\\n\\nPrompting\\n\\nAgents\\n\\nMultimodal\\n\\nSetup RAG Umgebung\\n\\nSetup künstliche Firmendatenbank\\n\\n12:30 - 13:30 Uhr: Mittagspause\\n\\nImplementieren eines RAG Prototypen mit den vorgestellten Komponenten\\n\\nDazwischen: Kaffeepausen um 15:00 Uhr und um 16:15 Uhr\\n\\nab 16:30: Test RAG Prototyp\\n\\nca. 17:00 Uhr: Ende\\n\\nTechnische Anforderungen\\n\\nBringt einen eigenen Laptop mit, der über mindestens 100 MB freien Festplattenspeicher (für Testdaten und Testdatenbank) verfügt.\\n\\nInstalliert vor dem Workshop eine Python-Umgebung (3.11) und erstellt eine virtuelle Umgebung (https://python.land/virtual-environments/virtualenv). Sie ist für alle Plattformen verfügbar. Ladet gerne bereits ein Setup Github Repository. Der Link wird vor dem Workshop verschickt.\\n\\nFalls ihr ein Gerät eurer Firma verwendet, überprüft vorher bitte, ob eines der folgenden, gelegentlich vorkommenden Probleme bei euch auftreten könnte. * Workshop-Teilnehmer:in hat keine Administrator-Rechte. * Corporate Laptops mit übermäßig penibler Sicherheitssoftware * Gesetzte Corporate-Proxies, über die man in der Firma kommunizieren muss, die aber in einer anderen Umgebung entsprechend nicht erreicht werden.\\n\\nSpeaker\\n\\nFabian Kaiser ist Experte für Generative AI. Er hat nach seinem Master in Informatik mit Fokus auf Natural Language Processing (NLP) ein Jahr lang am Ubiquitous Knowledge Processing (UKP) Lab zu Argument Mining in juristischen Texten geforscht. Seitdem hat er an diversen Projekten mit Fokus auf Textverarbeitung, IoT und Cloud Computing gearbeitet. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Python-Datenanalyse__Marimo_statt_Jupyter,_Polars_statt_Pandas,_Altair_statt_Matplotlib.md', 'filename': '../data/Python-Datenanalyse__Marimo_statt_Jupyter,_Polars_statt_Pandas,_Altair_statt_Matplotlib.md'}, page_content='Python-Datenanalyse: Marimo statt Jupyter, Polars statt Pandas, Altair statt\\n\\nMatplotlib In datengetriebenen Projekten spielt die richtige Visualisierung oft die entscheidende Rolle. Data Scientists erledigen das normalerweise in Jupyter- Notebooks und nutzen dazu die pandas-Bibliothek. Visualisierungen werden oft über matplotlib dargestellt. Doch Python bietet als Plattform weit mehr.\\n\\nDie neuen marimo-Notebooks beherrschen ein Abhängigkeitsmanagement und verhindern damit viele Fehler.\\n\\nDie polars-Bibliothek kann mit sehr großen Datenmengen umgehen und bietet viele Filtermöglichkeiten, die in pandas fehlen.\\n\\nAltair erlaubt eine deklarative Visualisierung und die elegante Kombination mehrere Diagramme.\\n\\nIn einem Beispielprojekt stellen wir die Zusammenarbeit dieser Komponenten vor.\\n\\nVorkenntnisse\\n\\nAlle Frameworks basieren auf Python, Vorkenntnisse sind daher wünschenswert. Die Plattform und die Frameworks sind für sich interessant, besonders spannend aber die Unterschiede zu den Standard-Tools (die man aber nicht kennen muss).\\n\\nLernziele\\n\\nReaktive Nutzung von marimo statt Jupyter\\n\\nVorteile von polars im Vergleich zu pandas\\n\\nInteraktion mit Altair und Widgets\\n\\nSpeaker\\n\\nChristian Winkler beschäftigt sich seit vielen Jahre mit künstlicher Intelligenz, speziell in der automatisierten Analyse natürlichsprachiger Texte (NLP). Als Professor an der TH Nürnberg konzentriert sich seine Forschung auf die Optimierung von User Experience mithilfe moderner Verfahren. Er forscht und publiziert zu AI/NLP und ist regelmäßig Sprecher auf Konferenzen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Eine_globale_Daten_&_AI_Platform_in_der_Praxis.md', 'filename': '../data/Eine_globale_Daten_&_AI_Platform_in_der_Praxis.md'}, page_content='Eine globale Daten & AI Platform in der Praxis\\n\\nMerck betreibt eine globale Daten & AI Platform, auf der täglich tausende Mitarbeiter in verschiedenen Rollen tätig sind. Dieser Vortrag ist ein Bericht aus der Praxis der letzten 5 Jahre, in dem wir, das Platform Team, unsere wichtigsten Lessons Learned und ihre technische Umsetzung vorstellen möchten:\\n\\nWarum sich Investments in das Sammeln von Metadaten über alle Aktivitäten auf der Platform schnell auszahlen.\\n\\nWie man es schafft, den Wert einer konzernweiten Daten & AI Platform – unabhängig von einzelnen Use Cases und Datenprodukten – zu quantifizieren.\\n\\nWas eine Balance zwischen Zentralisierung und Dezentralisierung bedeutet – und was Self-Service und Golden Paths damit zu tun haben.\\n\\nVorkenntnisse\\n\\nGrundkenntnisse und eigene Erfahrungen im Bereich von Datenarchitekturen und Data Mesh sind von Vorteil.\\n\\nLernziele\\n\\nWelche grundlegenden Erfolgsfaktoren eine Plattform resilient gegenüber kurzfristigen unternehmerischen Änderungen und fit für Innovationen wie GenAI machen.\\n\\nWie eine Ontology aller Plattform-Metadaten Themen wie FinOps, Governance und Security vorantreibt.\\n\\nWarum es essenziell ist, dass zentrale Teams nicht zu \"Bottlenecks\" werden.\\n\\nUmsetzung mit Technologien wie Palantir Foundry, AWS und Snowflake.\\n\\nSpeaker\\n\\nNicolas Renkamp verantwortet als Head of Platform Services die technische Umsetzung der Daten & AI Platform bei Merck Life Science. __@nicornk Florian Metz ist Product Owner der Merck-internen Daten & AI Platform. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/KI_zwischen_fantastischen_Möglichkeiten,_Rechtsunsicherheit_und_Disruption.md', 'filename': '../data/KI_zwischen_fantastischen_Möglichkeiten,_Rechtsunsicherheit_und_Disruption.md'}, page_content='KI zwischen fantastischen Möglichkeiten, Rechtsunsicherheit und Disruption\\n\\nChatGPT & Co. sind inzwischen in vielen Unternehmen im Bereich der produktiven Nutzung angelangt. Umso mehr stellt sich die Frage danach, wie denn der Einsatz dieser neuen Technik eigentlich rechtlich zu beurteilen ist:\\n\\nWie sieht es mit dem Urheberrecht an den Texten oder Bildern aus der KI aus?\\n\\nWas muss ich datenschutzrechtlich beachten?\\n\\nSind alle Ergebnisse einer KI wirklich frei an Urheberrechten Dritter?\\n\\nWelche Besonderheiten ergeben sich beim Coding mit KI?\\n\\nKann ich mich gegen die Nutzung meiner Werke durch die KI wehren?\\n\\nWas sollten Unternehmen bereits jetzt intern regeln?\\n\\nRechtsanwalt Niklas Mühleis stellt in seinem Vortrag am Beispiel des KI- Bilder-Tools Midjourney die aktuelle Rechtslage praxisnah dar und weist Ihnen Wege durch den Dschungel der juristischen Vorgaben.\\n\\nSpeaker\\n\\nNiklas Mühleis ist Rechtsanwalt und Partner bei der Kanzlei Heidrich Rechtsanwälte aus Hannover, die auf IT-Recht, Datenschutz und KI-Recht spezialisiert ist. Er hat es sich zur Aufgabe gemacht, technische und rechtliche Entwicklungen zu begleiten und fungiert dabei nicht selten als Dolmetscher zwischen den Sprachen der Technik und der Juristen. Er berät Mandanten vom kleinen Tech-Startup über den mittelständischen IT-Dienstleister bis hin zum Versicherungskonzern. Zudem veröffentlicht er seit Jahren Fachartikel in der c’t, iX und weiteren Zeitschriften, hält bundesweit Vorträge, engagiert sich bei der Förderung von Start-ups und hat einen Lehrauftrag für Datenschutzmanagement an der Hochschule Hannover. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Semantische_Suche_und_ML__Das_passende_Dokument_finden.md', 'filename': '../data/Semantische_Suche_und_ML__Das_passende_Dokument_finden.md'}, page_content='Semantische Suche und ML: Das passende Dokument finden\\n\\nDer Vortrag bietet eine Einführung in die Verwendung von maschinellem Lernen und vortrainierten Sprachmodellen für die Entwicklung von Suchsystemen mit semantischem Textverständnis.\\n\\nEr gibt einen Überblick über die technischen Entwicklungen der letzten Jahre, wie z.B. Text Embeddings und die Verwendung von LLMs, und diskutiert die verschiedenen Möglichkeiten, die heute zur Implementierung eines maßgeschneiderten Suchsystems zur Verfügung stehen.\\n\\nTeile der vorgestellten Methoden werden am Beispiel konkreter Kundenprojekte verdeutlicht.\\n\\nVorkenntnisse\\n\\nFür den Vortrag sind keine Vorkenntnisse nötig. Die mathematischen Details werden bewusst ausgelassen, um auch nicht technisch-versierten Personen einen Überblick über die modernen Methoden der Semantic Search zu geben.\\n\\nLernziele\\n\\nÜberblick über verschiedene Ansätze zum Textvergleich,\\n\\nVerständnis von Embedding und der Bewegung von Embeddings während des Trainings,\\n\\nEinblick in die Live-Anwendung der Domain-specific Semantic Search\\n\\nSpeaker\\n\\nJakob Scharlau ist ML Scientist bei der auf ML-Lösungen spezialisierten Softwareagentur dida in Berlin. Dort beschäftigt er sich hauptsächlich mit Themen aus dem Bereich Natural Language Processing. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Strategic_Data_Methods__A_Safari_Tour_for_Data_People.md', 'filename': '../data/Strategic_Data_Methods__A_Safari_Tour_for_Data_People.md'}, page_content='Strategic Data Methods: A Safari Tour for Data People\\n\\nAs data science and engineering work matures and evolves, our efforts become even more relevant to product-facing applications. Data scientists and engineers begin to work in a more cross-disciplinary environment. To facilitate this transition, we need non-technical tools in our toolkit. Fortunately, we can borrow more than enough from other fields.\\n\\nIn this talk, I’ll cover the most tried and tested strategic data methods. They include Design Thinking, Domain-Driven Design, Value Stream Mapping, and others. For each technique, I’ll describe the motivations behind its origin, and how it has been applied through the years in different contexts. We’ll then dive deeper into practical examples from a data context. Finally, I’ll provide a template you can reuse and adjust for your situation.\\n\\nTo get the most out of this talk, you should be a data scientist, analyst, or engineer who works at least semi-regularly with other teams, such as marketing or product.\\n\\nVorkenntnisse\\n\\nAttendees should have several years of experience in a product organization. Individual contributors will benefit from this talk, but ideally, they have experienced regular interactions with non-data team members (from other tech teams, product, marketing, etc.).\\n\\nLernziele\\n\\nBy getting exposure to diverse methods, attendees can conduct first sessions in their organizations. They will be able to customize the methods to the needs of their particular case. Finally, they will also be able to work more productively with members of adjacent teams, helping deliver better data- powered products for their organizations.\\n\\nSpeaker\\n\\nBoyan Angelov is a data strategist who has spent most of his career as a management consultant and CTO, working on designing and delivering data strategies for international organizations of all scales. He has written for O’Reilly and is the author of Elements of Data Strategy: A Framework for Data and AI-Driven Transformation. __@thinking_code Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Datenarchitekturen_in_der_Realität_–_Was_passt_bei_uns_.md', 'filename': '../data/Datenarchitekturen_in_der_Realität_–_Was_passt_bei_uns_.md'}, page_content='Datenarchitekturen in der Realität – Was passt bei uns?\\n\\nDie richtige Datenarchitektur und Plattform unterscheidet sich für jedes Unternehmen.\\n\\nWelche unterschiedlichen Wege gibt es?\\n\\nWelche Rahmenbedingungen beeinflussen Weg und Lösung?\\n\\nWollen alle die eine, magische Plattform oder gibt es auch sehr spitz zugeschnittene, spezifische Lösungen?\\n\\nAuch die Herausforderungen unterscheiden sich: Den einen fehlt Know-how und Kapazität, andere sind erschlagen von der Komplexität der eigenen IT- Landschaft, wieder andere müssen alles ohne Hilfe von Managed Services aufbauen.\\n\\nMit hohem Praxisbezug berichte ich von unterschiedlichen Projekten und Situationen – und zeige einen großen Teil der Bandbreite moderner Datenprojekte und der Herangehensweisen.\\n\\nVorkenntnisse\\n\\nErfahrungen mit Datenarchitekturen, Plattformen und Infrastruktur im Unternehmen sind hilfreich.\\n\\nLernziele\\n\\nÜberblick über die Lösungsmöglichkeiten für moderne Datenarchitektur\\n\\nVerstehen, welche Rahmenbedingungen in welcher Form die Architektur beeinflusst\\n\\nÜberblick über Entscheidungen, die beim Design der Architekturen getroffen werden, inklusive der Trade-Offs, die dabei üblicherweise zu berücksichtig sind.\\n\\nAnwendungen der verschiedenen Perspektiven im eigenen Umfeld\\n\\nSpeaker\\n\\nMatthias Niehoff unterstützt als Head of Data & AI der codecentric AG Kunden bei Design und Umsetzung von Datenarchitekturen. Dabei liegt sein Fokus auf der notwendigen Infrastruktur & Organisation, um Daten- & KI-Projekten zum Erfolg zu verhelfen. __@matthiasniehoff Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Datenanalysen_im_Fußball_–_ein_Überblick.md', 'filename': '../data/Datenanalysen_im_Fußball_–_ein_Überblick.md'}, page_content='Datenanalysen im Fußball – ein Überblick\\n\\nSeitdem die Moneyball-Philosophie das Baseball-Spiel revolutionierte, erleben wir in unterschiedlichen Sportarten teils gravierende Veränderungen – durch die zunehmende Verfügbarkeit von Daten und von Datenexpert:innen. Fußball ist allerdings immer noch eine gewisse Ausnahme, the beautiful game, mehr Kunst als Mechanik, einfacher zu genießen als zu analysieren.\\n\\nIn diesem Vortrags werden wir einen Blick werfen auf einige der Möglichkeiten und einige der Hindernisse, auf Daten und Analysen, auf Tools und natürlich \"AI\". Und wir werden Ähnlichkeiten und Unterschiede zwischen Fußballteams und Teams in modernen Unternehmen betrachten, vielleicht können wir ja vom Profisport auch etwas lernen über Zusammenarbeit, Leistung und Kultur.\\n\\nSpeaker\\n\\nStefan Kühn beschäftigt sich seit vielen Jahren mit Data Science, Machine Learning und mathematischer Grundlagenforschung. Nach Stationen bei codecentric, Zalando, XING, Tom Tailor und Snap Inc. fokussiert er sich in seiner jetzigen Rolle als VP Data & AI bei der air up GmbH auf Themen wie Data Strategy und Organisationsentwicklung, sowie das wichtigste Thema von allen - Data Quality. Darüber hinaus interessiert er sich vor allem für innovative Methoden im Kontext von Deep Learning und verantwortet das Datenscouting für den FC St. Pauli. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Datenkultur_bei_dm.md', 'filename': '../data/Datenkultur_bei_dm.md'}, page_content='Datenkultur bei dm\\n\\nWieso tun sich viele Unternehmen so schwer mehr von ihrer Datenbasis zu profitieren?\\n\\nAndreas Oehler berichtet über eine Reise bei dmTECH und dm-drogeriemarkt. Hier geht es um die Herausforderungen, welche Technologie, zentrale Datenteams und verteilte Datenentsorgung auf die nachhaltige Mehrwertgenerierung aus datenbasiertem Arbeiten haben.\\n\\nIn dem Vortrag wird von praktischen Erfahrungen bei der Etablierung der Data- Mesh-Prinzipien vor dem Hintergrund einer großen Drogeriemarktkette, welche starken Wert auf Eigenverantwortung legt und es bereits viele Erfahrungen mit DevOps-Best-Practices gibt.\\n\\nVorkenntnisse\\n\\nGrundlegendes Problemverständnis für analytische Datenarbeit und verwendete Infrastruktur.\\n\\nLernziele\\n\\nBesseres Verständnis der Herausforderungen bei Datenarbeit, bei der Anwendung der Data-Mesh-Prinzipien und einige Erkenntnisse was bei uns bisher gut funktioniert hat und was nicht.\\n\\nSpeaker\\n\\nAndreas Oehler arbeitet aktuell als Produktverantwortlicher für zwei Teams, die sich bei dmTECH mit analytischer Datenverarbeitung und -infrastruktur beschäftigen. Mit seinem naturwissenschaftlichen und IT- Engineering-Background bringt er sich schon seit mehreren Jahren in der Weiterentwicklung der unternehmensweiten Datenlandschaft ein. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Der_Mensch_im_Fokus__nutzerzentriert,_vertrauenswürdige_KI-Technologie_entwickeln.md', 'filename': '../data/Der_Mensch_im_Fokus__nutzerzentriert,_vertrauenswürdige_KI-Technologie_entwickeln.md'}, page_content='Der Mensch im Fokus: nutzerzentriert, vertrauenswürdige KI-Technologie\\n\\nentwickeln Leistungsstarke KI-Technologie gleicht häufig einer Blackbox, und das \"Warum?\" hinter ihren Entscheidungsprozessen einem Mysterium, selbst für ihre Entwickler:innen. Das stellt die Vertrauenswürdigkeit von KI-Systemen infrage.\\n\\nExplainable AI (XAI) findet Antworten – primär in algorithmischen Methoden, die komplexe Entscheidungsprozesse von KI-Systemen für Menschen verständlich machen. Die Richtung stimmt, beleuchtet das Problem aber nur eindimensional, denn Ihre Effektivität hängt ebenso davon ab, wie gut nicht-technische Nutzergruppen die Erklärungen verstehen.\\n\\nHuman-Centered Explainable AI stellt Endanwender:innen ins Zentrum der KI- Entwicklung. In interaktiven User Interfaces werden XAI-Methoden visualisiert und es den Nutzer:innen so ermöglicht, auf explorative Weise mit den KI- Modellen zu interagieren, um Einblicke in ihre Funktionsweise zu gewinnen und Vertrauen aufzubauen.\\n\\nWie sieht das in der Praxis aus?\\n\\nAm Beispiel eines ML-basierten Nachfrageprognose-Systems führt der Vortrag praxisnah durch die Wichtigkeit von User Research, die Selektion und Visualisierung von XAI-Methoden in einem User Interface. Ziel ist es, das Potenzial aufzuzeigen, KI-Entwicklung interdisziplinär zu denken, um vertrauenswürdige Lösungen zu bauen, die menschliche Kompetenz nicht ersetzen, sondern ergänzen.\\n\\nVorkenntnisse\\n\\nVorkenntnisse in Machine Learning, Explainable AI und UI/UX Design werden für das Verständnis des Vortrags nicht vorausgesetzt. Grundprinzipien, Design Pattern und XAI-Methoden werden erläutert.\\n\\nLernziele\\n\\nTeilnehmende verstehen die Relevanz von Explainable AI in Kombination mit einem menschenzentrierten Designprozess für die Entwicklung vertrauenswürdiger KI-Systeme. Der Zusammenhang von Erklärungen als integraler Bestandteil menschlicher Kognitionsprozesse und die Adaption auf XAI-Methoden wird vermittelt, und Zuhörer:innen erkennen das Potenzial von interaktiven User Interfaces für die effektive Gestaltung eines Mensch-KI-Dialogs.\\n\\nSpeaker\\n\\nAlina Döring ist UI/UX-Designerin bei inovex und studiert im Master \"Computer Science and Media\". Der Schwerpunkt ihrer Forschung liegt auf der Schnittstelle von User Experience Design und Machine Learning. Dabei untersucht sie die Gestaltung von Human-AI-Interaction durch Human-centered Explainable AI und integriert Erkenntnisse aus der Verhaltenspsychologie für einen holistischen Designprozess. Robin Senge ist Head of Machine Learning bei inovex. Er leitet ein Team von Data Scientists und Data Engineers und konzipiert als Spezialist für Maschinelles Lernen datengetriebene Use-Cases im Bereich Handel und Supply- Chain. Dr. Senge forscht aktiv im Bereich der Sicherheit von KI-Systemen sowie ihrer Interaktion mit dem Menschen durch Human-centered Explainable AI. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Die_Engineering-Perspektive_auf_den_EU_AI_Act.md', 'filename': '../data/Die_Engineering-Perspektive_auf_den_EU_AI_Act.md'}, page_content='Die Engineering-Perspektive auf den EU AI Act\\n\\nTrustworthy AI ist eine zentrale Motivation hinter dem EU AI Act. Dieser gesetzliche Rahmen legt vielschichtige und anspruchsvolle Anforderungen an KI- Systeme fest und kategorisiert sie in vier Risikogruppen: verbotene, hochriskante, begrenzt riskante KI-Systeme und solche mit minimalem Risiko.\\n\\nAngesichts der Schonfrist von 6 bis 24 Monaten nach Veröffentlichung der Verordnung wird empfohlen, dass Unternehmen, die KI einsetzen, mit der Vorbereitung auf die zukünftigen Anforderungen beginnen. Um die technische Bereitschaft für die Einhaltung des AI Act zu erreichen, ist es wesentlich, etablierte Data-Governance, KI-Governance und MLOps-Techniken über die gesamte Lebensdauer eines ML/AI-Systems anzuwenden.\\n\\nDieser Vortrag zielt darauf ab, Entwickler und Data Science Teams mit einem umfassenden Verständnis der Anforderungen des EU AI Act auszustatten. Er bietet auch praktische Anleitungen zur Implementierung von MLOps und Data- Governance-Prozessen, die der Verordnung entsprechen. Dieser Vortrag wird die Ingenieursperspektive auf die proaktive Implementierung des EU AI Act geben.\\n\\nVorkenntnisse\\n\\nErfahrung mit MLOps\\n\\nLernziele\\n\\nNach dem Talk sind ML Engineers in der Lage, proaktiv konforme MLOps-Prozesse und -Systeme zu entwerfen und zu implementieren und dabei die regulatorischen Anforderungen effektiv zu meistern.\\n\\nSpeaker\\n\\nLarysa Visengeriyeva ist Technologieberaterin und Expertin für Datenqualität, maschinelles Lernen und MLOps. Sie promovierte in Augmented Data Quality Management an der TU Berlin. Bei INNOQ.AI ist sie Head of Data and AI und arbeitet an der Operationalisierung von ML-Systemen und Datenarchitekturen. Larysa gründete das Sommerfestival Women+ in Data and AI. __@visenger Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Wieviel_KI_müssen_wir_unseren_Kunden_erklären_.md', 'filename': '../data/Wieviel_KI_müssen_wir_unseren_Kunden_erklären_.md'}, page_content='Wieviel KI müssen wir unseren Kunden erklären?\\n\\nWährend Menschen immer weiter von KI unterstützt werden und das Leben scheinbar einfacher wird, nimmt die technische Komplexität unserer Welt ständig zu. Wo vor zehn Jahren noch ein grundsätzliches Verständnis von Web- Technologien ausreichte, um \"das moderne Leben\" zu begreifen, nimmt die Halbwertszeit selbst von Expertenwissen in der KI immer weiter ab.\\n\\nFür uns gibt es Konferenzen. Doch wieviel KI muss der Fachbereich begreifen, um nicht komplett abgehängt zu werden? Von wie wenig Vorwissen müssen wir ausgehen, wenn wir Data Analytics und KI-Lösungen mit nichttechnischen Personen diskutieren? Egal ob interne Datenabteilung oder externer Dienstleister: Welche Erklärungen müssen folgen, wenn der Fachbereich denkt, dass wir etwas \"einfach mit KI lösen…\"?\\n\\nVorkenntnisse\\n\\nKeine Vorkenntnisse erforderlich. Lediglich ein Interesse an der Thematik, die über den technischen Aspekt hinausgeht.\\n\\nLernziele\\n\\nIch plane Impulse für all diejenigen zu geben, die mit nichttechnischen Personen über KI sprechen wollen und müssen: beim Tischgespräch in der Familie, beim Kundenpitch oder als Verantwortliche für Datenkultur, Change Management oder in der Politik.\\n\\nAnhand der Merck KGaA zeige ich, wie wir diese Themen den Fachbereichen näherbringen, etwa durch eine interne Datenakademie, in der Interessierte bis hin zum \"Citizen Data Scientist\" ausgebildet werden können.\\n\\nSpeaker\\n\\nBoris Adryan ist der Akademische Direktor der internen Data & Digital Academy beim chemisch-pharmazeutischen Unternehmen Merck KGaA. Dort plant und liefert er die Lerninhalte für alle Themen vom grundsätzlichen Datenverständnis bis hin zu generativer KI. Eigene Erfahrung sammelte er in einem Jahrzehnt akademischer Forschung im Bereich angewandter ML sowie als Data Scientist in der Industrie. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Size_recommender___a_data-driven_approach_to_fashion_sizing.md', 'filename': '../data/Size_recommender___a_data-driven_approach_to_fashion_sizing.md'}, page_content='Toggle navigation * Start * Die Konferenz * Programm * Location/Hotels * Tickets * Jetzt Ticket kaufen Ihr möchtet mit Eurem Team teilnehmen? Ab drei Personen profitiert Ihr von unseren Gruppenrabatten! Direkt im Shop buchen! Ihr möchtet mit Eurem Team teilnehmen? Ab drei Personen profitiert Ihr von unseren Gruppenrabatten! Direkt im Shop buchen! Die Konferenz für Data Scientists, Data Engineers und Data Teams Zurück\\n\\nSize recommender: a data-driven approach to fashion sizing\\n\\nOnline shopping has transformed fashion retail, offering convenience but posing challenges. At the forefront of it is the persistent problem of size returns, stemming from the limitations of online shopping where customers are unable to try before they buy.\\n\\nWe would like to introduce a data-driven size recommender to address this dilemma and enhance not only the cost efficiency but also customer satisfaction. By leveraging historical return data, we analyzed and labeled articles, discerning size patterns, and then employed machine learning to predict size labels. The performance was validated through rigorous A/B testing, and it showed remarkable success by a promising reduction in returns.\\n\\nVorkenntnisse\\n\\nA general understanding of data science, machine learning, and data-driven solutions in the context of retail would enable attendees to fully comprehend the content.\\n\\nLernziele\\n\\nIn this presentation, we delve into the journey of developing and implementing our data-driven size recommender. From the initial data analysis and labeling to the application of machine learning techniques for prediction, we outline a comprehensive process to improve the online shopping experience.\\n\\nSpeaker\\n\\nJie Bai is a Data Scientist at the center of excellence at E.Breuninger, developing the data-driven solutions that enable to deliver an outstanding shopping experience to customers. Before joining Breuninger, she worked mainly on academic research. She has a mechanical engineering and industrial engineering background, and holds Ph.D. in Operations Management. Jin Liu is a Data Scientist at E.Breuninger. After completed M.Sc in computer science she begins her journey in the realm of data science. She enjoys discovering insights from data and believes in the potential of data to be a guiding force in making informed choices that positively impact individuals and organizations. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Eine_Einführung_in_Large_Language_Models.md', 'filename': '../data/Eine_Einführung_in_Large_Language_Models.md'}, page_content='Eine Einführung in Large Language Models\\n\\nChatGPT ist fast schon zum Synonym für große Sprachmodelle geworden. Doch das ist nur die Spitze des Eisbergs – es gibt diese Modelle in viel mehr unterschiedlichen Ausprägungen.\\n\\nNach der Einführung der Transformer-Architektur durch Google gab es eine wahre Explosion an neuen Modellen. Grundsätzlich kann man zwischen Encoder- und (generativen) Decoder-Modellen unterscheiden, die sich für unterschiedliche Aufgaben eignen.\\n\\nDieser Vortrag gibt einen Überblick über die Architektur und die Entwicklung der unterschiedlichen Modelle. Er zeigt, dass sich Modelle auch leicht auf eigener Hardware oder mit kostenlosen Cloud-Diensten ausprobieren lassen und wie man das mit etwas Mühe auch ohne GPU auf der eigenen Hardware ausprobieren kann.\\n\\nDer Ausblick wagt eine Vorhersage, wie es mit den großen Sprachmodellen vielleicht weitergehen könnte.\\n\\nVorkenntnisse\\n\\nMit Sprachmodellen als Anwender hat vermutlich jeder/r schon gearbeitet. Grundkenntnisse in Machine Learning sind ebenso hilfreich wie ein Grundverständnis von CPUs, GPUs und Arbeitsspeicher.\\n\\nLernziele\\n\\nVerständnis der Architektur von großen Sprachmodellen\\n\\nUnterscheidung von Encoder- und Decoder-Modellen\\n\\nOptimierungsmöglichkeiten für Sprachmodelle\\n\\nKenntnis unterschiedlicher generativer Modelle\\n\\n\"Gefühl\" für die weitere Entwicklung\\n\\nSpeaker\\n\\nChristian Winkler beschäftigt sich seit vielen Jahre mit künstlicher Intelligenz, speziell in der automatisierten Analyse natürlichsprachiger Texte (NLP). Als Professor an der TH Nürnberg konzentriert er sich auf die Optimierung von User Experience mithilfe moderner Verfahren. Er forscht und publiziert zu Natural Language Processing und ist regelmäßig Sprecher auf Machine Learning-Konferenzen. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Urbane_Datenplattformen_für_Smarte_Städte.md', 'filename': '../data/Urbane_Datenplattformen_für_Smarte_Städte.md'}, page_content='Urbane Datenplattformen für Smarte Städte\\n\\nViele deutsche Städte und Regionen haben in den letzten Jahren Smart-City- Initiativen gestartet. Häufig bildet eine urbane Datenplattform einen zentralen Bestandteil dieser Projekte, um damit Daten, Datendienste und datenbasierte Anwendungen zu bündeln. Neben Datenintegration sehr heterogener Quellen, einem hohen Sicherheitsniveau, Governance, Datenbereitstellung und Visualisierung finden sich spezifischen Anforderungen: Das Datenmanagement soll nach FAIR-Prinzipien erfolgen, immer mehr Sensoren liefern Daten, und viele Daten haben einen Geo- Bezug. Wir möchten Herausforderungen sowie unsere Lösungsansätze anhand von Use Cases aus unseren Projekten vorstellen. Ein wenig KI ist auch dabei …\\n\\nVorkenntnisse\\n\\nGrundlagen Data Engineering und Architekturen für Datenplattformen\\n\\nLernziele\\n\\nAufbau modularer Datenplattformen\\n\\nUse Cases und Herausforderungen von smarten Städten und Regionen\\n\\nGeodaten-Integration\\n\\nDokumentenverarbeitung mit LLMs\\n\\nSpeaker\\n\\nStefan Igel ist COO bei Stackable und seit 25 Jahren im IT- Projektgeschäft. In dieser dieser Zeit hat Dr. Igel viele Unternehmen bei der Einführung von Datenplattformen begleitet. Er kann auf mehr als 12 Jahre Erfahrung in Big-Data-Projekten zurückblicken und koordiniert bei Stackable verschiedene Projekte im öffentlichen Bereich. Jonas Hein ist Leiter des Competence Centers Big Data & Advanced Analytik bei der Benz + Walter GmbH. Durch seine bisher über 10 Jahre Berufserfahrung, oft an der Schnittstelle zwischen Business und IT, besitzt er vielfältige Erfahrungen im Bereich von Datenplattformen, Smart City und Datenthemen. Aktuell ist er u. a. Projektleiter für die Umsetzung des DATEN:RAUM:FREIBURGs der Stadt Freiburg. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Mit_unFIX_zur_Data_Company.md', 'filename': '../data/Mit_unFIX_zur_Data_Company.md'}, page_content='Mit unFIX zur Data Company\\n\\nDie Herausforderungen bei der Umsetzung von Datenprojekten gehen weit über die technische Komponente – wie zum Beispiel das Training von Machine Learning Modellen – hinaus, und berühren organisatorische und kooperative Aspekte. Fragen nach den erforderlichen Strukturen, der Interaktion zwischen Datenexperten sowie den Eigenschaften von Daten-Teams sind zentral.\\n\\nKim und Oliver bündeln ihre Erfahrungen, um zu erörtern, wie Organisationen zu datengetriebenen Unternehmen werden können.\\n\\nNachdem das unFIX-Framework (eine Toolbox für dynamikrobustes Organisationsdesign) kurz eingeführt wurde, werden sie verschiedene damit designte Konstellationen von Data-Teams beleuchten und deren Vor- und Nachteile kritisch analysieren und erarbeiten, wann welche Konstellation am besten geeignet ist.\\n\\nVorkenntnisse\\n\\nGrundlegende Erfahrungen mit Datenarbeit\\n\\nLernziele\\n\\nEin Verständnis für die unterschiedlichen Rollen von Data Teams und wie in diesen gearbeitet wird.\\n\\nSpeaker\\n\\nOliver Zeigermann ist Software-Entwickler und -Architekt aus Hamburg. Er entwickelt seit 40 Jahren Software mit unterschiedlichen Ansätzen und Programmiersprachen. In letzter Zeit hat er sich vor allem mit Machine Learning und dessen Auswirkungen auf uns Menschen beschäftigt. __@DJCordhose Kim Nena Duggen ist bei der embarc Software Consulting GmbH als Organisations-Architektin im Bereich New Work, Selbstorganisation und (IT-)Strategie in ihrem Element, wenn sie mit Menschen arbeitet, die selbst etwas tun wollen, anstatt mit Theorie oder Musterlösungen vorliebzunehmen. Situationsgerecht wechselt sie in die Rolle der Beraterin, Coach oder Trainerin. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/LLM-MVC__Ein_Entwurfsmuster_zur_Strukturierung_von_LLM-Standardaufgaben_mit_generischen_Prompts.md', 'filename': '../data/LLM-MVC__Ein_Entwurfsmuster_zur_Strukturierung_von_LLM-Standardaufgaben_mit_generischen_Prompts.md'}, page_content='LLM-MVC: Ein Entwurfsmuster zur Strukturierung von LLM-Standardaufgaben mit\\n\\ngenerischen Prompts Systeme mit 100 oder mehr Prompts sind bei modernen LLM-Anwendungen nicht unüblich – eine Herausforderung in Bezug auf Wiederverwendbarkeit und Wartbarkeit. In diesem Vortrag stellen wir ein praxisnahes Entwurfsmuster für LLM-getrieben Prozesse vor, das in vielen Fällen helfen kann, Standardaufgaben klarer zu strukturieren und die Anzahl der benötigten Prompts stark zu verringern.\\n\\nUm das zu erreichen, übertragen wir die konzeptionellen Ideen hinter der klassischen Model-View-Controller-(MVC)-Architektur auf LLM-Prozesse. Die Schnittstellen zwischen den Komponenten werden dabei durch generische Prompts realisiert und können unabhängig vom Anwendungsfall wiederverwendet werden.\\n\\nVorkenntnisse\\n\\nWir setzen lediglich voraus, dass die Zuhörer mit den Standardbegriffen aus dem LLM-Umfeld vertraut sind, da diese nicht explizit erklärt werden.\\n\\nLernziele\\n\\nWir wollen exemplarisch zeigen wie man abseits von PoCs auf strukturierte Art und Weise mit LLMs arbeiten kann. Im Vortrag werden wir dabei auf folgende Aspekte genauer eingehen:\\n\\nWarum brauchen wir auch im LLM-Kontext standardisierte Entwurfsmuster\\n\\nWas sind generische Prompts und (indirekte) LLM-Prozesse\\n\\nWie kann man diese Konzepte verwenden um Aufgaben zu strukturieren und die Anzahl verschiedener Prompts zu reduzieren\\n\\nFür welche Anwendungsfälle ist das vorgestellte Entwurfsmuster nutzbar\\n\\nWelche Erfahrungen haben wir damit in der Praxis gesammelt (wir haben den Ansatz in mehrere große Produktivsysteme integriert)\\n\\nSpeaker\\n\\nRobert Bauer arbeitet als Senior Data Scientist bei der HMS Analytical Software GmbH im Bereich Data Science and Business Intelligence und ist dort u.a. als Lead Programmer für die Code-Migrations-Sparte zuständig. In dieser Rolle treibt er die Entwicklung verschiedener Inhouse-Tools zur Codemigration voran, u.a. die im Vortrag vorgestellten Lösungen für die Migration von SAS nach Python. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Schluss_mit_Prototyp__GPTx_und_RAG_in_der_Praxis.md', 'filename': '../data/Schluss_mit_Prototyp__GPTx_und_RAG_in_der_Praxis.md'}, page_content='Schluss mit Prototyp: GPTx und RAG in der Praxis\\n\\nDie Fähigkeiten der aktuellen GPT-Modelle zur Textgenerierung und Integration existierender Texte mittels Retrieval Augmented Generation (RAG) bieten ein riesiges Anwendungsspektrum.\\n\\nChristian und Oliver stellen ein innovatives Beispiel aus der Industrie vor und liefern eine Kiste voller praktischer Tipps für die konkrete Umsetzung, unter anderem:\\n\\nWelche Anwendungsbereiche können sinnvoll abgedeckt werden?\\n\\nVermeidung von Prompt-Injection, Umgang mit Halluzinationen und anderen Plattform \"Überraschungen\"\\n\\nWie Datenschutz und Geheimhaltung die Lösung mitprägen\\n\\nDieser Vortrag bietet praktische Einblicke für all jene, die die Möglichkeiten von GPT und RAG in eigenen Projekten nutzen möchten.\\n\\nVorkenntnisse\\n\\nEin Grundverständnis von Sprachmodellen\\n\\nLernziele\\n\\nEine Idee davon, was man für einen realistischen Einsatz von Sprachmodellen und RAG in Projekten braucht.\\n\\nSpeaker\\n\\nOliver Zeigermann ist Software-Entwickler und -Architekt aus Hamburg. Er entwickelt seit 40 Jahren Software mit unterschiedlichen Ansätzen und Programmiersprachen. In letzter Zeit hat er sich vor allem mit Machine Learning und dessen Auswirkungen auf uns Menschen beschäftigt. __@DJCordhose Christian Hidber arbeitet bei bSquare als Consultant mit Fokus auf Machine Learning, .Net und Azure. Nach seinem Mathematikstudium doktorierte er an der ETH Zürich und arbeitete als Postdoc am International Computer Science Institute in Berkeley. Aktuell arbeitet er am Einsatz von KI in der Geberit Planungssoftware ProPlanner. Jetzt Tickets sichern'),\n",
       " Document(metadata={'source': '../data/Integrating_Local_LLMs_and_Knowledge_Graphs_for_Trustworthy_Legal_Chatbots.md', 'filename': '../data/Integrating_Local_LLMs_and_Knowledge_Graphs_for_Trustworthy_Legal_Chatbots.md'}, page_content=\"Integrating Local LLMs and Knowledge Graphs for Trustworthy Legal Chatbots\\n\\nA specialised question-answering bot must prioritise robustness, trustworthiness and proper sourcing. While Large Language Models (LLMs) hold promise for improving chatbots, issues such as hallucinations and monolingual limitations pose challenges. In addition, concerns about data security hinder the adoption of hosted LLMs such as OpenAI in many organisations.\\n\\nOur German Law-bot addresses these challenges by integrating a local LLM with a knowledge graph (KG). We're evaluating several small local LLMs, building workflows incorporating KG retrieval, and refining models to ensure accurate and reliable legal answers. Our focus is on refining answers based on expert feedback.\\n\\nVorkenntnisse\\n\\nBasic knowledge of or interest in Large Language Models (LLMs) and Knowledge Graph (KG).\\n\\nLernziele\\n\\nCreate workflows that merge KG retrieval to improve QA bot capabilities.\\n\\nUnderstand the performance of local LLMs in German contexts.\\n\\nImplement expert feedback for iterative improvements.\\n\\nGain insight into AI-driven QA system development through interdisciplinary collaboration.\\n\\nLearn evaluation methods for assessing QA bot performance.\\n\\nSpeaker\\n\\nAnahita Pakiman is Senior Knowledge Graph-Data Scientist Consultant at brox IT-Solutions GmbH, developing graph-oriented retrieval systems and optimizing multi-agent interactions. Pursuing a Ph.D. in Applied AI with a focus on crash simulation knowledge graphs at Bergische Universität Wuppertal. Previously, Data Scientist at Fraunhofer SCAI specializing in knowledge graph development for vehicle processes. Christian Beil is Business Unit Lead at brox IT-Solutions GmbH, driving data and AI initiatives. Senior roles at medpex, Bonfire.com, ISO-Gruppe, and Scarus Software GmbH. Extensive Java expertise, leadership, and data quality analysis. Committed to innovation and excellence. Jetzt Tickets sichern\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating: 100%|██████████| 10/10 [03:02<00:00, 18.29s/it]     \n"
     ]
    }
   ],
   "source": [
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=azure_model, critic_llm=azure_model, embeddings=azure_embeddings\n",
    ")\n",
    "\n",
    "testset = generator.generate_with_langchain_docs(\n",
    "    documents,\n",
    "    test_size=10,\n",
    "    distributions={simple: 0.5, reasoning: 0.25, multi_context: 0.25},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wie bringt die interne Datenakademie der Merck...</td>\n",
       "      <td>[Wieviel KI müssen wir unseren Kunden erklären...</td>\n",
       "      <td>Die interne Datenakademie der Merck KGaA bring...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/Wieviel_KI_müssen_wir_uns...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wie hat die Einführung der Transformer-Archite...</td>\n",
       "      <td>[Eine Einführung in Large Language Models\\n\\nC...</td>\n",
       "      <td>Nach der Einführung der Transformer-Architektu...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/Eine_Einführung_in_Large_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wie können Data Scientists realistisches Erwar...</td>\n",
       "      <td>[KI Power-Play: Realistisches Erwartungsmanage...</td>\n",
       "      <td>Data Scientists können realistisches Erwartung...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/KI_Power-Play__Realistisc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wie tragen Data-Governance und MLOps-Techniken...</td>\n",
       "      <td>[Die Engineering-Perspektive auf den EU AI Act...</td>\n",
       "      <td>Data-Governance und MLOps-Techniken tragen zur...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/Die_Engineering-Perspekti...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wie wird die Sentimentanalyse mit LLMs im Work...</td>\n",
       "      <td>[Datenanalyse mit Machine Learning\\n\\nIn diese...</td>\n",
       "      <td>Die Sentimentanalyse wird im Workshop mit LLMs...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/Datenanalyse_mit_Machine_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Welche 4 Risikogruppen für KI-Systeme erforder...</td>\n",
       "      <td>[Die Engineering-Perspektive auf den EU AI Act...</td>\n",
       "      <td>Die vier Risikogruppen für KI-Systeme sind ver...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '../data/Die_Engineering-Perspekti...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Wie verbessert Christian Winkler die UX in NLP...</td>\n",
       "      <td>[Eine Einführung in Large Language Models\\n\\nC...</td>\n",
       "      <td>Christian Winkler konzentriert sich auf die Op...</td>\n",
       "      <td>reasoning</td>\n",
       "      <td>[{'source': '../data/Eine_Einführung_in_Large_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>How do data availability and explainable AI im...</td>\n",
       "      <td>[Datenanalysen im Fußball – ein Überblick\\n\\nS...</td>\n",
       "      <td>The context discusses the impact of data avail...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '../data/Datenanalysen_im_Fußball_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>How has Kira Engelhardt's finance background a...</td>\n",
       "      <td>[KI Power-Play: Realistisches Erwartungsmanage...</td>\n",
       "      <td>Kira Engelhardt's finance background and leade...</td>\n",
       "      <td>multi_context</td>\n",
       "      <td>[{'source': '../data/KI_Power-Play__Realistisc...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Welche technischen Anforderungen gibt es für T...</td>\n",
       "      <td>[Datenanalyse mit Machine Learning\\n\\nIn diese...</td>\n",
       "      <td>Die technischen Anforderungen für Teilnehmer, ...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': '../data/Datenanalyse_mit_Machine_...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  Wie bringt die interne Datenakademie der Merck...   \n",
       "1  Wie hat die Einführung der Transformer-Archite...   \n",
       "2  Wie können Data Scientists realistisches Erwar...   \n",
       "3  Wie tragen Data-Governance und MLOps-Techniken...   \n",
       "4  Wie wird die Sentimentanalyse mit LLMs im Work...   \n",
       "5  Welche 4 Risikogruppen für KI-Systeme erforder...   \n",
       "6  Wie verbessert Christian Winkler die UX in NLP...   \n",
       "7  How do data availability and explainable AI im...   \n",
       "8  How has Kira Engelhardt's finance background a...   \n",
       "9  Welche technischen Anforderungen gibt es für T...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [Wieviel KI müssen wir unseren Kunden erklären...   \n",
       "1  [Eine Einführung in Large Language Models\\n\\nC...   \n",
       "2  [KI Power-Play: Realistisches Erwartungsmanage...   \n",
       "3  [Die Engineering-Perspektive auf den EU AI Act...   \n",
       "4  [Datenanalyse mit Machine Learning\\n\\nIn diese...   \n",
       "5  [Die Engineering-Perspektive auf den EU AI Act...   \n",
       "6  [Eine Einführung in Large Language Models\\n\\nC...   \n",
       "7  [Datenanalysen im Fußball – ein Überblick\\n\\nS...   \n",
       "8  [KI Power-Play: Realistisches Erwartungsmanage...   \n",
       "9  [Datenanalyse mit Machine Learning\\n\\nIn diese...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  Die interne Datenakademie der Merck KGaA bring...         simple   \n",
       "1  Nach der Einführung der Transformer-Architektu...         simple   \n",
       "2  Data Scientists können realistisches Erwartung...         simple   \n",
       "3  Data-Governance und MLOps-Techniken tragen zur...         simple   \n",
       "4  Die Sentimentanalyse wird im Workshop mit LLMs...         simple   \n",
       "5  Die vier Risikogruppen für KI-Systeme sind ver...      reasoning   \n",
       "6  Christian Winkler konzentriert sich auf die Op...      reasoning   \n",
       "7  The context discusses the impact of data avail...  multi_context   \n",
       "8  Kira Engelhardt's finance background and leade...  multi_context   \n",
       "9  Die technischen Anforderungen für Teilnehmer, ...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': '../data/Wieviel_KI_müssen_wir_uns...          True  \n",
       "1  [{'source': '../data/Eine_Einführung_in_Large_...          True  \n",
       "2  [{'source': '../data/KI_Power-Play__Realistisc...          True  \n",
       "3  [{'source': '../data/Die_Engineering-Perspekti...          True  \n",
       "4  [{'source': '../data/Datenanalyse_mit_Machine_...          True  \n",
       "5  [{'source': '../data/Die_Engineering-Perspekti...          True  \n",
       "6  [{'source': '../data/Eine_Einführung_in_Large_...          True  \n",
       "7  [{'source': '../data/Datenanalysen_im_Fußball_...          True  \n",
       "8  [{'source': '../data/KI_Power-Play__Realistisc...          True  \n",
       "9  [{'source': '../data/Datenanalyse_mit_Machine_...          True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
