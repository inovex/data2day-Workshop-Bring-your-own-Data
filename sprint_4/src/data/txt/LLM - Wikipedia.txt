Ein '''{{lang|en|Large Language Model}}''', kurz '''{{lang|en|LLM}}''' (englisch, vereinzelt [[Lehnübertragung|übertragen]] '''großes Sprachmodell'''), ist ein [[Sprachmodell]], das sich durch seine Fähigkeit zur unspezifischen Erzeugung von Texten auszeichnet. Es handelt sich um ein [[Computerlinguistik|computerlinguistisches]] Wahrscheinlichkeitsmodell, das statistische Wort- und Satzfolge-Beziehungen aus einer Vielzahl von Textdokumenten durch einen rechenintensiven Trainingsprozess erlernt hat.

Große Sprachmodelle erlangen diese Fähigkeiten durch die Verwendung gigantischer Datenmengen, um während des Trainings riesige Mengen von Parametern zu lernen. Dabei verbrauchen sie extrem viele Rechenressourcen.<ref>{{Internetquelle |url=https://openai.com/research/better-language-models |titel=Better language models and their implications |sprache=en |abruf=2024-01-15}}</ref> Große Sprachmodelle sind im weiteren Sinne [[Künstliches neuronales Netz|künstliche neuronale Netze]] (im Grunde genommen sogenannte [[Transformer (Maschinelles Lernen)|Transformer]]<ref>{{Internetquelle |autor=Rick Merritt |url=https://blogs.nvidia.com/blog/what-is-a-transformer-model/ |titel=What Is a Transformer Model? |datum=2022-03-25 |sprache=en |abruf=2024-01-15}}</ref>) und werden ([[a priori]]) entweder durch selbst überwachtes Lernen oder halb überwachte Lernmethoden trainiert.

Große Sprachmodelle arbeiten als selbst anpassende Sprachmodelle, die ''verschiedene Aufgaben in natürlicher Sprache ausführen können, z.''&nbsp;''B. das Zusammenfassen, Übersetzen, Vorhersagen und Erstellen von Texten, indem sie einen Eingabetext nehmen und wiederholt das nächste Token oder Wort vorhersagen''.<ref>Guandong Feng, Guoliang Zhu, Shengze Shi, Yue Sun, Zhongyi Fan, Sulin Gao, and Jun Hu: Robust NL-to-Cypher Translation for KBQA: Harnessing Large Language Model with Chain of Prompts. In: Haofen Wang, Xianpei Han, Ming Liu, Gong Cheng, Yongbin Liu, Ningyu Zhang: ''Knowledge Graph and Semantic Computing: Knowledge Graph Empowers Artificial General Intelligence. 8th China Conference, CCKS 2023, Shenyang, China, August 24–27, 2023, Revised Selected Papers'' Springer, 2023, ISBN 978-981-9972-23-4, S. 317 ff. (hier [https://www.google.de/books/edition/Knowledge_Graph_and_Semantic_Computing_K/cMrfEAAAQBAJ?hl=de&gbpv=1&dq=%22predicting+the+next+token+or+word%22&pg=PA319&printsec=frontcover S. 319]) ("LLMs can perform various natural language tasks, such as understanding, summarizing, translating, predicting, and creating texts, by taking an input text and repeatedly predicting the next token or word"); vgl. [[arxiv:2304.00612|Eight Things to Know about Large Language Models]]</ref> Bis 2020 bestand die einzige Möglichkeit, ein Modell an bestimmte Aufgaben anzupassen, in der Feinabstimmung. Größere Modelle, wie z.&nbsp;B. das inzwischen populäre [[Generative Pre-trained Transformer 3|GPT-3]], wurden jedoch so konzipiert, dass sie mit Hilfe von [[Prompt Engineering]] ähnliche Ergebnisse erzielen können.<ref>{{Internetquelle |url=https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf |titel=Language Models are Few-Shot Learners |format=PDF |abruf=2024-01-15}}</ref> Zusätzlich zu der Fähigkeit, Kenntnisse über Syntax, Semantik und „Ontologie“ in menschlichen Sprachkorpora zu erwerben, wird angenommen, dass Große Sprachmodelle auch in der Lage sind, Ungenauigkeiten und Verzerrungen in den Korpora zu erfassen.<ref>{{Internetquelle |url=https://www.amacad.org/publication/human-language-understanding-reasoning |titel=Human Language Understanding & Reasoning |datum=2022-04-13 |sprache=en |abruf=2024-01-15}}</ref>

LLMs werden beispielsweise bei [[Open Assistant]], [[ChatGPT]], [[Ernie Bot]] und [[Grok]] eingesetzt. Einige große Sprachmodelle sind die GPT-Modellreihe von [[OpenAI]] (z.&nbsp;B. GPT-3.5 und GPT-4, die in ChatGPT und [[Microsoft Copilot]] verwendet werden), Googles [[PaLM]], [[Gemini (Sprachmodell)|Gemini]] (verwendet in Bard) und Gemma 2, [[LLaMA-Sprachmodell|Metas LLaMA-Familie]] von Open-Source-Modellen, [[Anthropic]]s [[Claude (Sprachmodell)|Claude]] und [[X.AI]]s Grok-1.

== Geschichte ==
Auf der „[[Conference on Neural Information Processing Systems]]“ (NeurIPS) 2017 stellten Google-Forscher unter [[Ashish Vaswani]] die Transformer-Architektur in ihrem Papier ''Attention Is All You Need'' vor.<ref>{{Internetquelle |autor=Ashish Vaswani et al |url=https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf |titel=Attention is all you need |hrsg=Google |format=PDF |sprache=en |abruf=2024-02-05}}</ref><ref>{{Internetquelle |autor=Rob Toews |url=https://www.forbes.com/sites/robtoews/2023/09/03/transformers-revolutionized-ai-what-will-replace-them/ |titel=Transformers Revolutionized AI. What Will Replace Them? |sprache=en |abruf=2024-02-05}}</ref> Ziel dieses Papiers war es, die [[Seq2seq]]-Technologie aus dem Jahr 2014 zu verbessern, und es basierte hauptsächlich auf dem von Bahdanau et al. 2014 entwickelten Aufmerksamkeitsmechanismus (attention mechanism).<ref>{{Internetquelle |autor=Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio |url=https://arxiv.org/abs/1409.0473 |titel=Neural Machine Translation by Jointly Learning to Align and Translate |werk=Arxiv |datum=2014-09-01 |sprache=en |abruf=2024-02-05}}</ref> Im darauffolgenden Jahr, 2018, wurde [[BERT Sprachmodell|BERT]] eingeführt und schnell „allgegenwärtig“. Obwohl der ursprüngliche Transformator sowohl [[Kodierer|Encoder]]- als auch Decoderblöcke hat, ist BERT ein reines Encoder-Modell.<ref>[//www.bigdata-insider.de/was-ist-bert-a-1116088/ Was ist BERT?] – von ''Stefan Luber'', über ''Bigdata-Insider'', am 10. Mai 2022</ref>

Obwohl GPT-1 im Jahr 2018 als reines Decoder-Modell eingeführt wurde, erregte [[OpenAI|GPT-2]] im Jahr 2019 große Aufmerksamkeit, da [[OpenAI]] es zunächst als zu leistungsfähig erachtete, um es aus Angst vor böswilliger Nutzung zu veröffentlichen. [[Generative Pre-trained Transformer 3|GPT-3]] im Jahr 2020 ging noch einen Schritt weiter und ist ab 2024 nur noch über eine API verfügbar, ohne die Möglichkeit, das Modell zur lokalen Ausführung herunterzuladen. Es war das browserbasierte ChatGPT aus dem Jahr 2022, das „die Welt komplett veränderte“.<ref>{{Internetquelle |url=https://www.euronews.com/next/2023/11/30/chatgpt-a-year-on-3-ways-the-ai-chatbot-has-completely-changed-the-world-in-12-months |titel=ChatGPT turns 1: How the AI chatbot has completely changed the world |datum=2023-11-30 |sprache=en |abruf=2024-02-05}}</ref> 2023 wurde GPT-4 für seine erhöhte Genauigkeit und als „heiliger Gral“ für seine multimodalen Fähigkeiten gepriesen.<ref>{{Internetquelle |url=https://www.technologyreview.com/2023/03/14/1069823/gpt-4-is-bigger-and-better-chatgpt-openai/ |titel=GPT-4 is bigger and better than ChatGPT—but OpenAI won’t say why |sprache=en |abruf=2024-02-05}}</ref> OpenAI gab die High-Level-Architektur und die Anzahl der Parameter des GPT-4 nicht bekannt.

In der Zwischenzeit haben konkurrierende Sprachmodelle größtenteils mit der GPT-Serie gleichgezogen, zumindest was die Anzahl der Parameter betrifft.<ref>{{Internetquelle |url=https://ourworldindata.org/grapher/artificial-intelligence-parameter-count?time=2017-09-05..latest |titel=Parameters in notable artificial intelligence systems |hrsg=''[[Our World in Data]]'' |datum=2024-04-03<!-- Last updated --> |abruf=2024-05-16 |sprache=en-GB}}</ref> Zu den bemerkenswerten Ausnahmen in Bezug auf die Anzahl der Parameter gehören Googles T5-11B von 2019 und [[PaLM|PaLM-E]] von 2022. Am 26. Januar 2024 übertraf Googles Gemini Pro GPT-4,<ref>{{Internetquelle |autor=Siddharth Jindal |url=https://analyticsindiamag.com/googles-gemini-pro-beats-gpt-4/ |titel=Google's Gemini Pro Beats GPT-4 |datum=2024-01-27 |sprache=en-US |abruf=2024-02-05}}</ref> was die [[Elo-Zahl|Elo-Bewertung]] betrifft.

Seit 2022 erfreuen sich quell verfügbare Modelle zunehmender Beliebtheit, zunächst vor allem BLOOM und [[LLaMA]], die allerdings beide Einschränkungen im Einsatzbereich aufweisen. Im Januar 2024 war Mixtral 8x7b von [[Mistral AI]] laut dem LMSYS Chatbot Arena Leaderboard das leistungsfähigste offene LLM, leistungsfähiger als GPT-3.5, aber nicht so leistungsfähig wie GPT-4.<ref>{{Internetquelle |url=https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard |titel=LMSys Chatbot Arena Leaderboard - a Hugging Face Space by lmsys |abruf=2024-02-05}}</ref>

=== Hardware ===
Seit einigen Jahren gibt es Chiparchitekturen, die für das Training und Inferencing von großen Sprachmodellen optimiert sind. 2016 wurde beispielsweise von Google die erste Version ihrer [[Tensor Processing Unit|TPU]] vorgestellt. Seit den 2020er Jahren gibt es aber eine ganze Reihe von Herstellern mit Spezial-Hardware für die Bearbeitung von LLMs. So haben beispielsweise [[Cerebras]] den CS-1 und CS-2, AMD die Instinct Serie, Intel die Gaudi-Plattform und [[Nvidia]] Hopper bzw. dessen Nachfolger [[Blackwell (Grafikprozessor)|Blackwell]] eingeführt bzw. angekündigt.

== Multimodal Learning ==
{{Hauptartikel|Multimodale künstliche Intelligenz}}
Multimodal Learning verwendet verschieden strukturierte Daten im Bereich der [[Künstliche Intelligenz|künstlichen Intelligenz]]:<ref>{{Internetquelle |autor= |url=https://datascientest.com/de/multimodal-learning-die-technik-die-die-kuenstliche-intelligenz-revolutioniert |titel=Multimodal Learning: Die Technik, die die künstliche Intelligenz revolutioniert |werk=Weiterbildung Data Science {{!}} DataScientest.com |datum=2023-08-17 |sprache=de-DE |abruf=2024-06-24}}</ref>

* Text ist eine der am häufigsten verwendeten Modalitäten im [[Maschinelles Lernen|maschinellen Lernen]]. Textdaten enthalten strukturierte Informationen, und mithilfe der [[Natürliche Sprachverarbeitung|natürlichen Sprachverarbeitung]] lässt sich leicht Wissen aus ihnen extrahieren. Die Techniken, die zur Verarbeitung dieser Informationen verwendet werden, umfassen [[Tokenisierung]], [[Lemmatisierung]], [[Syntaxanalyse]], Erkennung von benannten Entitäten und Textklassifizierung.
* Bilder sind eine wesentliche Quelle visueller Informationen. Mithilfe von [[Convolutional Neural Network|Convolutional Neural Networks]] konnten große Fortschritte beim Verständnis von Bildern erzielt werden. Verwendete Techniken sind z. B. die [[Objekterkennung]], die [[Gesichtserkennung]] und die Segmentierung von Bildern.
* Die Audiomodalität umfasst Informationen aus Sprachaufnahmen, Tondateien oder Live-Streams.
* Videos sind eine leistungsstarke Quelle für multimodale Daten, weil sie visuelle und auditive Informationen kombinieren. [[Computer Vision]] und Audioverarbeitungstechniken ermöglichen es, Wissen aus einer Videosequenz zu extrahieren. Dies ermöglicht die Erkennung von sich bewegenden Objekten, die Analyse menschlicher Aktivitäten oder sogar die Erkennung von Gesten.

== Bootstrapping Language-Image Pretraining ==
Die meisten modernen Vision-Language-Modelle verursachen während des Vortrainings einen hohen Rechenaufwand, weil das Training mit umfangreichen Modellen und Datensätzen erfolgt. Die Forschung befindet sich an der Schnittstelle zwischen Sehen und Sprache. Daher ist zu erwarten, dass Vision-Language-Modelle von den leicht verfügbaren unimodalen Modellen der [[Bilderkennung]] und natürlichen [[Spracherkennung]] profitieren können.

Vortrainierte Vision-Modelle bieten eine qualitativ hochwertige visuelle Darstellung. Vortrainierte [[Sprachmodell|Sprachmodelle]], insbesondere große Sprachmodelle, bieten leistungsstarke Fähigkeiten zur Sprachgenerierung und Zero-Shot-Übertragung. Um die Kosten zu senken und dem Problem des katastrophalen Vergessens entgegenzuwirken, bleiben die unimodalen vortrainierten Modelle während des Vortrainings eingefroren. Weil große Sprachmodelle jedoch während ihres unimodalen Vortrainings keine Bilder gesehen haben, macht das Einfrieren die visuelle Sprachausrichtung besonders schwierig.<ref>{{Literatur |Autor=Junnan Li, Dongxu Li, Silvio Savarese, Steven Hoi |Titel=BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models |Datum=2023 |arXiv=2301.12597 |DOI=10.48550/ARXIV.2301.12597}}</ref>

== Skalierungsgesetze ==
Passt man für jede Modalität <math>j</math> die sieben Parameter der [[Gleichung]]

: <math>\mathcal{L}(N, D_j) = E_j + \frac{A_j}{N^{\alpha_j}} + \frac{B_j}{|D_j|^{\beta_j}}  </math>

an und minimiert

: <math>\sum_{i \mod j} H_{\sigma = 0.03} [ LSE(a_j - \alpha_j \cdot \log(N_i), b - \beta \cdot \log(D_i), e_j) - L_i ] </math>

für <math>\{a_j, b_j, e_j, \alpha_j, \beta_j\}</math>, wobei <math>H </math> der Standard-Huberverlust für jeden Durchlauf <math>i</math> und Modalität <math>j</math> ist. Man setz dann <math>A_j = e^{a_j} </math>, <math>B_j = e^{b_j} </math>, <math>E_j = e^{e_j} </math>. Um die optimalen Minima zu identifizieren, verwendet man das [[BFGS-Verfahren|BGFS-Verfahren]] auf demselben Gitter der Initialisierungswerte. Die erhaltenen optimalen Werte befinden sich nicht an den Grenzen des Initialisierungsgitters. Die Skalierungsgesetze für jede Modalität sind im Einzelnachweis verfügbar. Die Parameter für jede Modalität variieren erheblich.<ref>{{Literatur |Autor=Armen Aghajanyan, Lili Yu, Alexis Conneau, Wei-Ning Hsu, Karen Hambardzumyan, Susan Zhang, Stephen Roller, Naman Goyal, Omer Levy, Luke Zettlemoyer |Titel=Scaling Laws for Generative Mixed-Modal Language Models |Datum=2023-01-10 |DOI=10.48550/ARXIV.2301.03728}}</ref>

== Kollaps ==
Bei LLM und [[Foundation Models]] anderer Art (VAE, GMM) kann es durch das andauernde Training in der laufenden Nutzung zur dauerhaften, bis zur Unbrauchbarkeit reichenden, Verschlechterung der Ergebnisqualität kommen ([[Modellkollaps]], ''model collapse''). Dies kann auch nachfolgende Modellversionen betreffen, die mit einem zunehmenden Anteil künstlich generierter Trainingsdaten erstellt werden, da eine Vorsortierung in der Regel durch [[Webscraping]] erlangter Daten bisher als zu aufwändig erscheint.<ref>{{Literatur |Autor=Ilia Shumailov, Zakhar Shumaylov, Yiren Zhao, Nicolas Papernot, Ross Anderson, Yarin Gal |Titel=AI models collapse when trained on recursively generated data |Sammelwerk=Nature |Band=631 |Nummer=8022 |Datum=2024-07-25 |ISSN=0028-0836 |DOI=10.1038/s41586-024-07566-y |PMC=11269175 |PMID=39048682 |Seiten=755–759 |Online=https://www.nature.com/articles/s41586-024-07566-y |Abruf=2024-07-27}}</ref>

== Trivia ==
Um [[Fehlinformationseffekt|Misinformation]] durch mit Webinhalten trainierte LLM vorzubeugen, schlugen Forschende der [[Stanford University|Stanford-Universität]] 2024 ''WikiChat'' vor, ein vorrangig Wikipedia als Wissensbasis nutzendes Sprachmodell.<ref>{{Literatur |Autor=Sina J. Semnani, Violet Z. Yao, Heidi C. Zhang, Monica S. Lam |Titel=WikiChat: Stopping the Hallucination of Large Language Model Chatbots by Few-Shot Grounding on Wikipedia |Datum=2023 |arXiv=2305.14292 |DOI=10.48550/ARXIV.2305.14292}}</ref> Eine GPT-4-Implementierung habe demnach höhere inhaltliche Richtigkeit aufgewiesen als GPT-4 allein.<ref>{{Internetquelle |url=https://github.com/stanford-oval/WikiChat |titel=WikiChat |werk=Open Virtual Assistant Lab |hrsg=Stanford University |sprache=en |abruf=2024-02-06}}</ref>

== Siehe auch ==
* [[Generativer vortrainierter Transformer]] (GPT)
* [[Language Model for Dialogue Applications]] (LaMDA)

== Weblinks ==
* [https://www.iese.fraunhofer.de/blog/large-language-models-ki-sprachmodelle/ iese.fraunhofer.de: ''Was sind Large Language Models? Und was ist bei der Nutzung von KI-Sprachmodellen zu beachten?'']

== Einzelnachweise ==
<references />

{{Normdaten|TYP=s|GND=1322631905}}

{{SORTIERUNG:Grosses Sprachmodell}}
[[Kategorie:Künstliche Neuronale Netze]]
[[Kategorie:Computerlinguistik]]

[[bar:Large language model]]
