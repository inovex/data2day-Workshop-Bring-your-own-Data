{
 "cells": [
  {
   "cell_type": "code",
   "id": "d98c8e1c-eb7d-4ad4-8864-353252f4f65e",
   "metadata": {},
   "source": [
    "import os\n",
    "from time import sleep\n",
    "\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from datasets import Dataset\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings\n",
    "from phoenix.session.evaluation import get_qa_with_reference\n",
    "from phoenix.trace import SpanEvaluations, using_project\n",
    "from phoenix.trace.langchain import LangChainInstrumentor\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall,\n",
    "    faithfulness, answer_correctness,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7e33c700-76d8-4004-89b0-dc4e842fbaa2",
   "metadata": {},
   "source": [
    "# load .env file\n",
    "load_dotenv()\n",
    "\n",
    "# get azure credentials from .env file\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_VERSION\")\n",
    "deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT\")\n",
    "embedding_deployment_name = os.getenv(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\")\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "temperature = os.getenv(\"TEMPERATURE\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "96f1bdea-6303-44b6-bef5-668a223ee5ab",
   "metadata": {},
   "source": [
    "def build_chain(embeddings_model, model):\n",
    "    # load vectorstore\n",
    "    vectorstore = Chroma(embedding_function=embeddings_model, persist_directory=\"./chroma_db\")\n",
    "\n",
    "    # initialize a retriever from the vectorstore\n",
    "    retriever = vectorstore.as_retriever()\n",
    "\n",
    "    # crate a system prompt that tells the LLM to answer questions based on the given context\n",
    "    # and use a variable that represents the context\n",
    "    system_prompt = (\n",
    "        \"You are an assistant for question-answering tasks. \"\n",
    "        \"Use the following pieces of retrieved context to answer \"\n",
    "        \"the question. If you don't know the answer, say that you \"\n",
    "        \"don't know. Use three sentences maximum and keep the \"\n",
    "        \"answer concise.\"\n",
    "        \"\\n\\n\"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    # create a prompt template with the system prompt\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # create a helper chain that inserts the retrieved documents into the prompt\n",
    "    question_answer_chain = create_stuff_documents_chain(model, prompt)\n",
    "\n",
    "    # create the final RAG chai\n",
    "    chain = create_retrieval_chain(retriever, question_answer_chain)\n",
    "\n",
    "    return chain"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4877e01-f6d2-4412-8030-3e3bb5674aab",
   "metadata": {},
   "source": [
    "def generate_ragas_dataset(chain, test_data_df):\n",
    "    test_questions = test_data_df[\"question\"].values\n",
    "\n",
    "    # execute chain and store answers and retrieved context\n",
    "    responses = [\n",
    "        chain.invoke({\"input\": question})\n",
    "        for question in test_questions\n",
    "    ]\n",
    "\n",
    "    contexts = []\n",
    "    for response in responses:\n",
    "        page_contents = [doc.page_content for doc in response[\"context\"]]\n",
    "        contexts.append(page_contents)\n",
    "\n",
    "    test_data_df[\"answer\"] = [response[\"answer\"] for response in responses]\n",
    "    test_data_df[\"contexts\"] = contexts\n",
    "    test_dataset = Dataset.from_pandas(test_data_df)\n",
    "\n",
    "    return test_dataset"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "14acf392-fcad-4d2e-8759-4fdf9ad7da43",
   "metadata": {},
   "source": [
    "# initialize the Embedding Model\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    api_version=api_version,\n",
    "    openai_api_type='azure',\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    azure_deployment=embedding_deployment_name,\n",
    ")\n",
    "\n",
    "# initialize the Azure OpenAI Model\n",
    "model = AzureChatOpenAI(\n",
    "    azure_endpoint=azure_endpoint,\n",
    "    deployment_name=deployment_name,\n",
    "    api_key=api_key,\n",
    "    api_version=api_version,\n",
    "    openai_api_type=\"azure\",\n",
    "    temperature=0.0,\n",
    "    streaming=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a63212ca-4796-49df-8220-6eebdd7b7c8d",
   "metadata": {},
   "source": [
    "# create a chain\n",
    "chain = build_chain(embeddings, model)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4314cfb4-27b8-4f8e-bf4c-c8b38ad2a0bd",
   "metadata": {},
   "source": [
    "# read in the testset as a dataframe\n",
    "evaluation_data = pd.read_csv(\n",
    "    \"testdata.csv\",\n",
    "    usecols=[\"question\", \"ground_truth\"],\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a53a7f2e-e5c3-4dff-ab8a-ec458b61a44c",
   "metadata": {},
   "source": [
    "# start phoenix session and client\n",
    "session = px.launch_app(use_temp_dir=False)\n",
    "client = px.Client()\n",
    "\n",
    "# initialize Langchain auto-instrumentation\n",
    "LangChainInstrumentor().instrument()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94f48f03-46a0-4062-b6f3-7230b90fd8f1",
   "metadata": {},
   "source": [
    "# create ragas testset in an extra project\n",
    "with using_project(\"test\"):\n",
    "    ragas_eval_dataset = generate_ragas_dataset(chain, evaluation_data)\n",
    "\n",
    "ragas_evals_df = pd.DataFrame(ragas_eval_dataset)\n",
    "\n",
    "ragas_evals_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3c364102-45e9-4a6b-9dab-bd904f96c022",
   "metadata": {},
   "source": [
    "# wait a few seconds in case data hasn't become fully available yet\n",
    "sleep(5)\n",
    "\n",
    "# collect information about rag spans\n",
    "spans_dataframe = get_qa_with_reference(client, project_name=\"test\")\n",
    "\n",
    "spans_dataframe"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3dbcc0c4-0b5b-42c3-9b1c-fa8ae2d66975",
   "metadata": {},
   "source": [
    "# use extra project to see how ragas works under the hood\n",
    "with using_project(\"ragas-evals\"):\n",
    "    # start evaluation\n",
    "    evaluation_result = evaluate(\n",
    "        dataset=ragas_eval_dataset,\n",
    "        metrics=[faithfulness, answer_correctness, context_recall, context_precision],\n",
    "        llm=model,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "# get evaluation scores\n",
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)\n",
    "\n",
    "# get evaluation data\n",
    "eval_data_df = pd.DataFrame(evaluation_result.dataset)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1d83c3bb-a23d-4bc7-a0e0-03f221c4bbfd",
   "metadata": {},
   "source": [
    "# assign span ids to the ragas evaluation scores (needed so Phoenix knows where to attach the spans).\n",
    "span_questions = (\n",
    "    spans_dataframe[[\"input\"]]\n",
    "    .sort_values(\"input\")\n",
    "    .drop_duplicates(subset=[\"input\"], keep=\"first\")\n",
    "    .reset_index()\n",
    "    .rename({\"input\": \"question\"}, axis=1)\n",
    ")\n",
    "\n",
    "ragas_evals_df = ragas_evals_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\n",
    "eval_data_df = eval_data_df.merge(span_questions, on=\"question\").set_index(\"context.span_id\")\n",
    "eval_scores_df.index = eval_data_df.index"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa529981-80a4-4ba6-bac1-29d2cc5818f5",
   "metadata": {},
   "source": [
    "for eval_name in eval_scores_df.columns:\n",
    "    evals_df = eval_scores_df[[eval_name]].rename(columns={eval_name: \"score\"})\n",
    "    evals = SpanEvaluations(eval_name, evals_df)\n",
    "    px.Client().log_evaluations(evals)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0bd5b445-57b7-4e9d-a420-2c570bfba357",
   "metadata": {},
   "source": [
    "px.close_app()"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
